{"categories":[{"title":"Gin","uri":"https://zhangshunping.github.io/categories/gin/"},{"title":"k8s","uri":"https://zhangshunping.github.io/categories/k8s/"},{"title":"linux","uri":"https://zhangshunping.github.io/categories/linux/"},{"title":"operator","uri":"https://zhangshunping.github.io/categories/operator/"},{"title":"redis","uri":"https://zhangshunping.github.io/categories/redis/"},{"title":"个人开源小工具","uri":"https://zhangshunping.github.io/categories/%E4%B8%AA%E4%BA%BA%E5%BC%80%E6%BA%90%E5%B0%8F%E5%B7%A5%E5%85%B7/"},{"title":"云原生","uri":"https://zhangshunping.github.io/categories/%E4%BA%91%E5%8E%9F%E7%94%9F/"},{"title":"日志","uri":"https://zhangshunping.github.io/categories/%E6%97%A5%E5%BF%97/"},{"title":"监控","uri":"https://zhangshunping.github.io/categories/%E7%9B%91%E6%8E%A7/"},{"title":"语言-golang","uri":"https://zhangshunping.github.io/categories/%E8%AF%AD%E8%A8%80-golang/"}],"posts":[{"content":"[toc]\nRedis 特点   Value有类型，自带本地计算方法（数据运算）\n  多路复用模型（redis 优先选则epoll模型）\n  woker单线程，i/o threads 多线程（redis 6.x版本以上）\n  Value自带本地方法 redis的value是多类型且自带本地方法，这与memc样，memchache的值类型只有字符串。比如发送请求获取一个list的key的value，redis可以在本地调用index（）计算，而memcache只能够返回一段json，只能客户端从着端json里先反序列化，然后获取对应的key。这也是为什么memcache比redis早，却流行多不如redis的原因之一吧。\n多路复用模型 i/O 多路复用模型是利用select、poll、epoll可以同时监察多个流的 I/O 事件的能力，在空闲的时候，会把当前线程阻塞掉，当有一个或多个流有I/O事件时，就从阻塞态中唤醒，于是程序就会轮询一遍所有的流（epoll是只轮询那些真正发出了事件的流），依次顺序的处理就绪的流，这种做法就避免了大量的无用操作。这里“多路”指的是多个网络连接，“复用”指的是复用同一个线程。采用多路 I/O 复用技术可以让单个线程高效的处理多个连接请求（尽量减少网络IO的时间消耗），且Redis在内存中操作数据的速度非常快（内存内的操作不会成为这里的性能瓶颈），主要以上两点造就了Redis具有很高的吞吐量。\n点击跳转i/o复用理解\n单线程 worker  为什么worker要单线程，不使用多线程\n 从一个全局角度去俯视一个web网站架构图,对数据DB的进行Write操作的时候，无论前面的server有多少在并行，处理数据一定是串行，如果是多线程，必然要考虑加锁解锁的过程，加锁的本质也是一个串行流程，因此从这点来看，redis选择单线程的模式，反而比采用多线程的模式相对而言，不会出现复杂算法问题，且效率较高。redis实际上是采用了线程封闭的观念，把任务封闭在一个线程，自然避免了线程安全问题，不过对于需要依赖多个redis操作的复合操作来说，依然需要锁，而且有可能是分布式锁，\n这么一来不当紧，因为redis是单线程的，一个redis服务，**worker进程只能够跑在一个cpu上，因此cpu的资源使用率会带来浪费吗？**带着着疑问我们来看一下，如果redis 是一个worker线程去处理数据请求的过程是怎样的？\n如果只有一个woker线程的话，一个请求过来，worker线程首先要读取请求，然后计算数据 ,最后写入返回结果。当一个请求处理完之后，依次处理第二个请求。这样一来，我们会发现如果采用这种机制，redis处理请求的方式一定是不连续的，因为计算第一请求的时候，i/o处理会等待redis计算时间。\n因此redis,采用i/o threads的机制。用i/o threads线程来处理 i/o请求，用worker进程来计算。这样几乎可以认为是连续的响应请求。\nRedis(6.X版本的I/O threads模式是多线程模式）\n拓展下:其实这种worker+I/O threads也是微服务架构设计的思想之一。\nRedis 值类型  写在前面\n Redis是采用二进制安全的机制。二进制安全是一种主要用于字符串操作函数相关的计算机编程术语。一个二进制安全功能（函数），其本质上将操作输入作为原始的、无任何特殊格式意义的数据流。对于每个字符都公平对待，不特殊处理某一个字符。\n 举个栗子：C语言中的字符串是根据特殊字符“\\0”来判断该字符串是否结束，对于字符串str=\u0026ldquo;0123456789\\0123456789”来说，在C语言里面str的长度就是10（strlen(str)=10），所以strlen()函数不是二进制安全的。而在Redis中，strlen str的结果是21，是二进制安全的（Redis底层所使用的字符串表示是Sds），它只关心二进制化的字符串，不关心字符串的具体格式，里面有啥字符，只会严格的按照二进制的数据存取，不会以某种特殊格式解析字符串。\n ","id":0,"section":"posts","summary":"[toc] Redis 特点 Value有类型，自带本地计算方法（数据运算） 多路复用模型（redis 优先选则epoll模型） woker单线程，i/o threads 多线程（re","tags":null,"title":"Redis","uri":"https://zhangshunping.github.io/2021/02/redis/","year":"2021"},{"content":"k8s官网介绍 一、网站均衡负载入口策略  架构图\n  架构图说明\n k8s对外暴露使用ingress-nginx-controoler网关的方式对外暴露服务。\n用户登录域名，通过阿里云 DNS服务器解析，一般A记录解析 k8s集群上的ingress-nginx-controller网关所在的k8s节点的外网ip，k8s 集群内的ingress-nginx-controller以daemonset的方式运行在集群节点上，listen 80和443端口；CNAME记录则解析到对应的CDN服务。\ningress-nginx 部署方式如下：\n[root@master01 ingress]# kubectl get daemonset -ningress-nginx NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE admin-ingress-nginx-controller 3 3 3 3 3 ml-api-ingress-nginx-controller 5 5 5 5 5 ml-static-ingress-nginx-controller 3 3 3 3 3 zgame-ingress-nginx-controller 2 2 2 2 2 ## admin-ingress-nginx-controller 管理员网关（用作日志，监控的服务注册） ## ml-api-ingress-nginx-controller ml服务的api网关 ## ml-static-ingress-nginx-controller ml服务的静态网关 ## zgame-ingress-nginx-controller zame服务网关  当通过A记录解析，把流量分发到k8s对应的ingress-controller网关之后，网关会解析域名，分发到对应的ingress资源上去，然后ingress将流量分发到对应的service上去，service通过k8s原生的endpoint资源将流量转发到真实的pod服务上去。\n 举个栗子： 当请求访问 ”api.XXXXXX.com“时候，首先经过阿里云DNS服务解析（阿里云A记录如图），假如A记录解析的IP为：\u0026ldquo;X.x.x.202\u0026rdquo;,这个时候流量就会导入到外网地址为\u0026quot;X.x.x.202\u0026quot;这台k8s节点上.与此同时ingress-nginx-controller是以damoneset的方式运行，通过采用hostnetwork的方式共享宿主机网络命名空间，所以这个时候listen的80的端口正是admin-ingress-nginx-controller控制器产生的pod为\u0026quot;ml-api-ingress-nginx-controller-7tscl\u0026quot;的ingress-controller pod服务，这个时候流量就会导入到这个pod服务内。这个ingress-controller pod会根据上面注册的ingress，将流量通过service代理导入到提供服务的pod。从而实现一次流量转发.如下图：\n Tips: 这里的入口流量的均衡负载方式，是采用DNS均衡负载方式，流量直接导入到k8s集群对应的公网节点上。因此如果有k8s节点下线，需要注意清理阿里云的A记录  ingress-controoler资源常用操作命令\n#1. 查看ingress-controller 网关 kubectl get pods -ningress-nginx #2. 查看ingress资源 ##目前常用项目资源分别分布在 mlweb,mgameweb,zgameweb,elastic-system等命令空间下 kubectl get ingress -nmlweb kubectl get ingress -nzgameweb    1.1网络策略   目前只针对 monitoring 空间下的ingress做了白名单，具体可通过如下命令查看对应的资源\n[root@master01 config]# kubectl get ingress access-grafana -nmonitoring -oyaml --- *** nginx.ingress.kubernetes.io/whitelist-source-range: 180.169.85.114/32,137.59.103.18/32,103.206.188.183/32,101.230.210.194/32,161.202.217.208/32,161.202.217.167/32 ---     二、Jenkins CI/CD发布流程  Jenkins 实现 kubernetes的官网版本发布流程。\n  jenkins官网应用发布更新流程详情\n 1.拉取svn代码   官网版本管理 是通过\n第一种：\u0026ldquo;版本类型tags+\u0026ldquo;版本release\u0026rdquo;+\u0026ldquo;project\u0026rdquo;+\u0026ldquo;站点site\u0026rdquo;+\u0026ldquo;location\u0026rdquo;\n第二种: \u0026ldquo;版本类型truck\u0026rdquo;+\u0026ldquo;project\u0026rdquo;+\u0026ldquo;站点site\u0026rdquo;+\u0026ldquo;location\u0026rdquo; 这两种方式来管理对应的分支代码。\n第一种可以认为是迭代版本;第二种认为是新分支,及不存在release版本号。因此拉取代码的时候，要明确知道tags+release+project+site+localtion。\n举个栗子：\njenkins 名为 \u0026ldquo;k8s-ml-dev-api.xxxx.com job\u0026quot;的job 执行自动发布流程，需要输入对应的参数。\njenkins用户输入：\n然后jenkins执行如下脚本，进行版本发布：\n  ## 执行shell array=(${release//\\// }) release=${array[0]} type=`echo ${type}|sed 's/-//'` bash -x /usr/local/src/k8s-dev-deploy.sh \u0026quot;api.xxxxxxxxxx.com\u0026quot; \u0026quot;$location\u0026quot; \u0026quot;$release\u0026quot; true \u0026quot;$type\u0026quot; \u0026quot;${BUILD_NUMBER}\u0026quot; \u0026quot;${branch}  2.编译代码，并打包docker镜像 2.1 编译前准备 代码编译分为**：静态服务和非静态服务**\n  1。如果静态服务编译，需要在本地安装对应的静态依赖包\nif [ \u0026quot;${dynamic}\u0026quot; == \u0026quot;false\u0026quot; ];then if [ -f \u0026quot;webpack.config.js\u0026quot; ] || [ -f \u0026quot;vue.config.js\u0026quot; ] || [ -f \u0026quot;package.json\u0026quot; ];then [ -d \u0026quot;dist\u0026quot; ] \u0026amp;\u0026amp; rm -rf dist [ -d \u0026quot;node_modules\u0026quot; ] \u0026amp;\u0026amp; rm -rf node_modules grep node-sass package.json \u0026gt;/dev/null [ $? -eq 0 ] \u0026amp;\u0026amp; /usr/local/node/bin/npm install node-sass --unsafe-perm /usr/local/node/bin/npm install [ $? -ne 0 ] \u0026amp;\u0026amp; exit 1 /usr/local/node/bin/npm run ${project_env} [ $? -ne 0 ] \u0026amp;\u0026amp; exit 1 fi fi    2.非静态地址则需要，替换MFW地址和DB地址\ncd ${SOURCE_DIR}/${project}/${site}${location} sed -i \u0026quot;s#\\(MFW_LOCATOR=\\).*#\\1\\\u0026quot;$MFW_LOCATOR_ADDR\\\u0026quot;#g\u0026quot; ${project_env_file} sed -i \u0026quot;s#\\(DB_HOST=\\).*#\\1${DB_HOST_ADDR}#g\u0026quot; ${project_env_file} sed -i \u0026quot;s#\\(DB_HOST_MLWEB=\\).*#\\1${DB_HOST_ADDR}#g\u0026quot; ${project_env_file} sed -i \u0026quot;s#\\(DB_HOST_HERO=\\).*#\\1${DB_HOST_ADDR}#g\u0026quot; ${project_env_file} sed -i \u0026quot;s#\\(DB_HOST_PLAYERCARE=\\).*#\\1${DB_HOST_ADDR}#g\u0026quot; ${project_env_file} sed -i \u0026quot;s#\\(DB_HOST_ML_WEB=\\).*#\\1${DB_HOST_ADDR}#g\u0026quot; ${project_env_file} ls .[!.]* [ $? -eq 0 ] \u0026amp;\u0026amp; tar -zcf ${app_name}-${release}.tar.gz * .[!.]* || tar -zcf ${app_name}-${release}.tar.gz * rm -rf $DOCKER_IMGAE_FACTORY/SOURCES/${app_name}-${release}.tar.gz mv ${app_name}-${release}.tar.gz $DOCKER_IMGAE_FACTORY/SOURCES/    2.2 正式编译打包  将编译文件目录拷贝到本地docker编译目录，根据不同类型生成dockerfile文件，编程生成docker镜像，上传到docker仓库服务  3.生成k8s资源清单,并apply创建资源 3.1 跟调度有关   应用pod通过deployment 控制器进行管理，通过标签选择器nodeSelector选择调度\n## ml项目常见选择器 nodeSelector: kubernetes.io/project: ml kubernetes.io/node-role: gateway kubernetes.io/gateway: api nodeSelector: kubernetes.io/project: ml kubernetes.io/node-role: gateway kubernetes.io/gateway: static    3.2 跟存储有关  应用pod的只针对了 应用日志和nginx日志做持久化,业务本身是无状态的。  4.修改site对应的ingress资源. 一般操作如下\npaths=`kubectl --kubeconfig=${KUBE_CONFIG} --insecure-skip-tls-verify -n ${NAMESAPCES} get ingress ${domain} -o jsonpath='{.spec.rules[0].http.paths..path}'` if [ $? -eq 0 ];then location_path=\u0026quot;$location(/|$)(.*)\u0026quot; [ \u0026quot;$location\u0026quot; == \u0026quot;/\u0026quot; ] \u0026amp;\u0026amp; location_path=\u0026quot;/\u0026quot; if [ \u0026quot;$paths\u0026quot; == \u0026quot;\u0026quot; ];then patch=\u0026quot;[{\\\u0026quot;op\\\u0026quot;: \\\u0026quot;add\\\u0026quot;, \\\u0026quot;path\\\u0026quot;: \\\u0026quot;/spec/rules/0/http\\\u0026quot;, \\\u0026quot;value\\\u0026quot;:{\\\u0026quot;paths\\\u0026quot;:[{\\\u0026quot;path\\\u0026quot;:\\\u0026quot;${location_path}\\\u0026quot;,\\\u0026quot;backend\\\u0026quot;:{\\\u0026quot;serviceName\\\u0026quot;:\\\u0026quot;${app_name,,}\\\u0026quot;,\\\u0026quot;servicePort\\\u0026quot;:80}}]}}]\u0026quot; kubectl --kubeconfig=${KUBE_CONFIG} --insecure-skip-tls-verify -n ${NAMESAPCES} patch ingress ${domain} --type json -p \u0026quot;$patch\u0026quot; else echo \u0026quot;$paths\u0026quot; |grep -w \u0026quot;$location\u0026quot; \u0026gt;/dev/null if [ $? -ne 0 ];then patch=\u0026quot;[{\\\u0026quot;op\\\u0026quot;: \\\u0026quot;add\\\u0026quot;, \\\u0026quot;path\\\u0026quot;: \\\u0026quot;/spec/rules/0/http/paths/-\\\u0026quot;, \\\u0026quot;value\\\u0026quot;:{\\\u0026quot;path\\\u0026quot;:\\\u0026quot;${location_path}\\\u0026quot;,\\\u0026quot;backend\\\u0026quot;:{\\\u0026quot;serviceName\\\u0026quot;:\\\u0026quot;${app_name,,}\\\u0026quot;,\\\u0026quot;servicePort\\\u0026quot;:80}}}]\u0026quot; kubectl --kubeconfig=${KUBE_CONFIG} --insecure-skip-tls-verify -n ${NAMESAPCES} patch ingress ${domain} --type json -p \u0026quot;$patch\u0026quot; fi fi fi  ","id":1,"section":"posts","summary":"k8s官网介绍 一、网站均衡负载入口策略 架构图 架构图说明 k8s对外暴露使用ingress-nginx-controoler网关的方式对外暴露服","tags":["kubernetes"],"title":"CI/CD kubernetes落地","uri":"https://zhangshunping.github.io/2021/02/kubernetes-cicd%E8%90%BD%E5%9C%B0/","year":"2021"},{"content":"官网日志架构 一、ECK方案介绍 官网日志方案目前使用的Elastic Cloud on Kubernetes (ECK) operator 的方案在kubernetes集群内落地日志服务。ECK方案落地文档\nECK方案部署Elastic Statck架构，在k8s主要注册了四个主要 CRD：APM、ElasticSearch、Kibana,beat\n 可以通过以下命令查看\n [root@master01 1.3.1]# kubectl get crd|grep elastic #1.apm apmservers.apm.k8s.elastic.co 2020-11-13T07:26:30Z #2.filebeat beats.beat.k8s.elastic.co 2020-12-23T08:42:52Z #3.elasticsearch elasticsearches.elasticsearch.k8s.elastic.co 2020-11-13T07:26:30Z enterprisesearches.enterprisesearch.k8s.elastic.co 2020-12-23T08:42:52Z #4.kibana kibanas.kibana.k8s.elastic.co 2020-11-13T07:26:30Z  ECK Operator拉起一个Controller，通过list/watch自定义的CRD资源做出对应的action，从而实现对ElstaticSearch+kibana+beats+apm等集群的管理.\n 可以在集群内通过以下命令查看Controller\n [root@master01 1.3.1]# kubectl get StatefulSet -nelastic-system NAME READY AGE elastic-operator 1/1 95d  二、官网日志方案 官网日志落地方案基于ECK方案上做了个性化，认证方面通过syncladp自研组件同步moa账号，实现账号管理。采用ECK方案原生的方案落地ElasticSearch集群和Kibana。采用自定义Demonset控制器落地Filebeat采集agent（没有使用ECK自带的beat资源）。\n 日志方案架构图\n 2.1 Filebeat Filebeat是通过daemonset控制器的方式在每一个k8s节点上以pod的方式运行，主要收集 k8s各个节点上的**/data/applog/nginx-ingress**和**/data/applog/nginx**下的日志文件。\n以收集**/data/applog/nginx-ingress/**日志为列，这里需要注意的是：\n  业务pod，需要将业务日志（这里大多是nginx的日志log），挂载到宿主机上的**/data/applog/nginx-ingress**下\n  filebeat pod，需要将宿主机的**/data/applog/nginx-ingress**挂在到pod内\n  filebeat跟elasticsearch通信，基于kubernetes 内的servicename通信。\noutput.elasticsearch: hosts: [\u0026quot;website-es-http:9200\u0026quot;] protocol: \u0026quot;https    目前收集的Zagme,Ml,Mgame项目业务的日志\n  index命名规则？\n  2.2 EleasticSearch 集群 因为目前的日志量不是很大，因此采用的是filebeat采集的数据直接录入到elasticSearch的方式。官网ES集群落地是采用ECK原生的构建CRD资源方式。可以通过mastr01节点上路径/usr/local/src/back/config/eck/1.3.1/Elasticsearch.yaml查看EleasticSearch资源清单详情。\n2.2.1 查看ElasticSearch 状态 执行以下命令查看ElasticSearch资源\n[root@master01 1.3.1]# kubectl get elasticsearch -nelastic-system NAME HEALTH NODES VERSION PHASE AGE website green 3 7.10.1 Ready 48d   Nodes 代表着EleaticSearch的集群大小 HEALTH代表着集群的健康状态  green状态：每个索引的primary shard和replica shard都是active状态 yellow ： 每个索引的primary shard都是active状态，但是部分replica shard不是active状态，处于不可用状态 red: 不是所有的索引的primary shard都是active状态，部分索引有数据丢失了    2.2.1 ElasticSearch 配置说明  ElasticSearch结合LDAP实现权限、用户管控  xpack: security: authc: realms: ldap: ldap1: order: 0 url: \u0026quot;XXXXX.XXXX.XXXX\u0026quot; # 配置的openladp+syncladp的地址（svc地址) timeout: tcp_connect: \u0026quot;120s\u0026quot; tcp_read: \u0026quot;120s\u0026quot; ldap_search: \u0026quot;120s\u0026quot; bind_dn: \u0026quot;XXX.XXX.XXX\u0026quot; ##ldap管理账户dn user_search: base_dn: \u0026quot;ou=People,ou=project-128,dc=XX,dc=net\u0026quot; #在这个目录树里面检索用户信息 filter: \u0026quot;(uid={0})\u0026quot; group_search: base_dn: \u0026quot;ou=Group,ou=project-128,dc=XX,dc=net\u0026quot; # 在这个目录树里面检索组信息 files: role_mapping: \u0026quot;/usr/share/elasticsearch/config/role_mapping.yml\u0026quot; # 下面这个是通过文件方式配置的LDAP用户域和Es内置角色的映射关系 unmapped_groups_as_roles: false    openladp+syncladp服务可以通过以下命令查看\n[root@master01 1.3.1]# kubectl get pods -nkube-system openldap-manage NAME READY STATUS RESTARTS AGE openldap-manage 2/2 Running 0 72d [root@master01 1.3.1]# kubectl get svc -nkube-system openldap-server NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE openldap-server ClusterIP 172.31.230.127 \u0026lt;none\u0026gt; 389/TCP 84d    ElasticSearch 存储跟Prometheus集群一样，采用了Openebs的方案做的localpv存储\nvolumeClaimTemplates: - metadata: name: elasticsearch-data spec: accessModes: - ReadWriteOnce resources: requests: storage: 100Gi storageClassName: openebs-hostpath    2.3 Kibana kibana是日志方案的web界面，落地方案也是采用ECK原生的CRD资源进行管理落地。\n2.3.1 查看Kibana资源 [root@master01 1.3.1]# kubectl get kibana -nelastic-system NAME HEALTH NODES VERSION AGE website green 1 7.10.1 48d  2.3.2 kibana 服务暴露方式 kibana服务暴露是通过ingress的方式注册打ingress-nginx网关上进行服务暴露,可以通过如下命令查看\n[root@master01 1.3.1]# kubectl get ingress -nelastic-system NAME HOSTS ADDRESS PORTS AGE kibana-ingress kibana.k8s.moonton.net 10.142.87.198,10.142.87.209,10.142.87.216 80, 443 96d  Tips: kibana ingress跟grafana一样，设置了白名单\nnginx.ingress.kubernetes.io/whitelist-source-range: 180.169.85.114/32,137.59.103.18/32,103.206.188.183/32,101.230.210.194/32,161.202.217.208/32,161.202.217.167/32  2.4 Wathcer报警方式 日志告警，是通过Xpack 插件提供的告警模块Watcher提供的报警规则.wathcer官方操作手册，为了配置一个 Watcher，我们必须配置如下的几个配置：\nPUT _watcher/watch/my-justme-watcher { \u0026quot;trigger\u0026quot;: {}, \u0026quot;input\u0026quot;: {}, \u0026quot;condition\u0026quot;: {}, \u0026quot;actions\u0026quot;: {} }  trigger \u0026quot;trigger\u0026quot;: { \u0026quot;schedule\u0026quot; : { \u0026quot;interval\u0026quot;: \u0026quot;1h\u0026quot; } },  上面表明，我们希望 watcher 每一个小时运行一次。针对我们的情况，我们定义为：\n\u0026quot;trigger\u0026quot;: { \u0026quot;schedule\u0026quot;: { \u0026quot;cron\u0026quot;: \u0026quot;0 0/1 * * * \u0026quot; } },  上面表明，我们希望每一分钟运行一次。\ninput \u0026quot;input\u0026quot;: { \u0026quot;search\u0026quot;: { \u0026quot;request\u0026quot;: { \u0026quot;indices\u0026quot;: [ \u0026quot;filebeat-*\u0026quot; ], \u0026quot;body\u0026quot;: { \u0026quot;query\u0026quot;: { \u0026quot;bool\u0026quot;: { \u0026quot;must\u0026quot;: { \u0026quot;match\u0026quot;: { \u0026quot;process.name\u0026quot;: \u0026quot;JUSTME\u0026quot; } }, \u0026quot;filter\u0026quot;: { \u0026quot;range\u0026quot;: { \u0026quot;@timestamp\u0026quot;: { \u0026quot;from\u0026quot;: \u0026quot;{{ctx.trigger.scheduled_time}}||-5m\u0026quot;, \u0026quot;to\u0026quot;: \u0026quot;{{ctx.trigger.triggered_time}}\u0026quot; } } } } } } } } },  在上面，我们想针对 filebeat-* 的索引来进行搜索。如果 process.name 含有 JUSTME，则表明有一个匹配。同时我们针对在过去5分钟内发生的事件来进行查询。\ncondition 我们定义如下：\n\u0026quot;condition\u0026quot; : { \u0026quot;compare\u0026quot; : { \u0026quot;ctx.payload.hits.total\u0026quot; : { \u0026quot;gt\u0026quot; : 0 }} },  上面显示如果在查询结果中，有至少一个的结果，那么就会发送出alert,ctx.payload.hits.total表示查询结果\nactions 这个是定义当上面的 condition 被满足时，需要采取的一些行动。这些行动可以在地址找到。目前 Watcher 支持的action为: email，``webhook，``index，``logging，``slack 及``pagerduty。\n官网报警配置 可以通过kibana查看。\n具体规则可以根据trigger，input，condition，actions的配置规则查看\n\u0026mdash;-未完待续\n","id":2,"section":"posts","summary":"官网日志架构 一、ECK方案介绍 官网日志方案目前使用的Elastic Cloud on Kubernetes (ECK) operator 的方案在kubernetes集群内落地日志服务。ECK方案落地文","tags":["kubernetes","operator"],"title":"Elastic Statck Operator","uri":"https://zhangshunping.github.io/2021/02/elasticsearch-operator%E8%90%BD%E5%9C%B0/","year":"2021"},{"content":"账号管理 Elastic Stack日志方案和Prometheus监控系统的用户权限管理，都是通过接入 openladp同步自研MOA管理系统，统一管理账号，架构图如下：\n 架构说明\n 1.openldap+syncldap 服务以pod的方式运行再k8s集群内。syncldap服务是自研服务，他的作用是定时从自研MOA鉴权服务里同步账号信息并转换为openldap能识别的的信息。可以在集群内通过如下命令查看\n[root@master01 grafana]# kubectl get pod/openldap-manage -nkube-system NAME READY STATUS RESTARTS AGE openldap-manage 2/2 Running 0 50d [root@master01 grafana]# kubectl get svc -nkube-system openldap-server ClusterIP 172.31.230.127 \u0026lt;none\u0026gt; 389/TCP 62d  2.日志服务（EFK）中，elasticserach通过xpack配置ldap服务，从而实现ldap统一管理\n3.mertics监控，grafana服务接入ldap服务，实现账号统一管理\n官网监控方案 原生Promtheus监控方案  Prometheus监控方案架构图如下\n 在上面流程中**，Prometheus**通过配置文件中指定的**服务发现方式**来确定要**拉取**监控指标的目标（Target），接着从要拉取的目标（**应用容器和Pushgateway**）发起HTTP请求到特定的端点（Metric Path），将指标持久化至本身的**TSDB**中，TSDB最终会把内存中的时间序列压缩落到硬盘，除此之外，Prometheus会定期通过PromQL计算设置好的告警规则，决定是否生成告警到**Alertmanager**，后者接收到告警后会负责把通**知发送到邮件或企业内部群聊中**。\n由上可知，一套prometheus方案落地，则需要用到以下几个重要服务组件：\nPrometheus Server: 收集指标和存储时间序列数据，并提供查询接口 Push Gateway: 短期存储指标数据。主要用于临时性的任务 **Exporters:**采集已有的第三方服务监控指标并暴露metrics Alertmanager:告警 Web UI :简单的web控制台\nkube-prometheus方案是prometheus的Operator开源方案，将以上重要的服务自定义为kubernetes资源（CRD），通过自定义控制器的方式实现对prometheus监控方案的管理。\nkube-prometheus介绍 kube-prometheus是通过k8s operator的方式实现的监控方案，首先会在k8s集群内注册多个crd资源，其中最主要四种的crd资源分别是：\n Prometheus: 由 Operator 依据一个自定义资源kind: Prometheus类型中，所描述的内容而部署的 Prometheus Server 集群，可以将这个自定义资源看作是一种特别用来管理Prometheus Server的StatefulSets资源。 ServiceMonitor: 一个Kubernetes自定义资源(和kind: Prometheus一样是CRD)，该资源描述了Prometheus Server的Target列表，Operator 会监听这个资源的变化来动态的更新Prometheus Server的Scrape targets并让prometheus server去reload配置(prometheus有对应reload的http接口/-/reload)。而该资源主要通过Selector来依据 Labels 选取对应的Service的endpoints，并让 Prometheus Server 通过 Service 进行拉取（拉）指标资料(也就是metrics信息)，metrics信息要在http的url输出符合metrics格式的信息，ServiceMonitor也可以定义目标的metrics的url。 Alertmanager：Prometheus Operator 不只是提供 Prometheus Server 管理与部署，也包含了 AlertManager，并且一样通过一个 kind: Alertmanager 自定义资源来描述信息，再由 Operator 依据描述内容部署 Alertmanager 集群。 PrometheusRule:对于Prometheus而言，在原生的管理方式上，我们需要手动创建Prometheus的告警文件，并且通过在Prometheus配置中声明式的加载。而在Prometheus Operator模式中，告警规则也编程一个通过Kubernetes API 声明式创建的一个资源.告警规则创建成功后，通过在Prometheus中使用想servicemonitor那样用ruleSelector通过label匹配选择需要关联的PrometheusRule即可。  然后会在集群里以deployment的方式运行kube-prometheus的controller自定义控制器。这个控制器会定时的list/watch上述的四种主要的crd资源，通过loop循环的方式来调谐管理对应的资源，从而有效的管理监控资源。\n可以通过如下命令查看controller控制器和对应的crd资源\n[root@master01 kube-prometheus]# kubectl get deployment -nmonitoring NAME READY UP-TO-DATE AVAILABLE AGE prometheus-operator 1/1 1 1 75d ## 主要crd资源 [root@master01 manifests]# kubectl get servicemonitor -nmonitoring NAME AGE alertmanager 74d coredns 74d [root@master01 manifests]# kubectl get alertmanagers -nmonitoring NAME VERSION REPLICAS AGE main v0.21.0 1 71d [root@master01 manifests]# kubectl get prometheusrules -nmonitoring NAME AGE ingress-nginx-monitor-rules 71d prometheus-k8s-rules 74d pxc-monitor-rules 54d [root@master01 manifests]# kubectl get prometheus -nmonitoring NAME VERSION REPLICAS AGE k8s v2.22.1 1 50d  kube-prometheus管理prometheus监控大致流程图如下：\nkube-promethues日常管理 写在前面：以下提到的运维操作都是基于 kube-prometheus方案落地的promtheus监控体系阐述一些运维工作。\nkube-prometheus项目是通过注册CRD资源的方式，因此管理Prometheus集群，我们是通过promethues-promethues.yaml 文件的资源定义管理prometheus。目前官网配置可以通过查看master01节点上的/usr/local/src/back/config/kube-prometheus/manifests/prometheus/prometheus-prometheus.yaml 文件\n prometheus-prometheus.yml配置说明\n apiVersion: monitoring.coreos.com/v1 kind: Prometheus metadata: labels: prometheus: k8s name: k8s namespace: monitoring spec: ##1.通过 additional的方式添加prometheus.yaml配置文件内容 additionalScrapeConfigs: name: additional-configs key: prometheus-additional.yaml ##2. pvc 申请pv，用作Prometheus存储 storage: volumeClaimTemplate: spec: storageClassName: openebs-hostpath resources: requests: storage: 100Gi ##3.配置altermanger地址 alerting: alertmanagers: - name: alertmanager-main namespace: monitoring port: web image: quay.io/prometheus/prometheus:v2.24.0 ... ... ##控制prometheus的集群规模 replicas: 2 ... ... ## prometheus报警规则配置 ruleSelector: matchLabels: prometheus: k8s role: alert-rules .... ....  一、监控项 1.1 添加监控项 由上可知，kube-prometheus方案，是通过ServiceMonitor，主要通过Selector来依据 Labels 选取对应的Service的endpoints，并让 Prometheus Server 通过 Service 进行拉取（拉）指标资料(也就是metrics信息)。因此创建一个自定义监控一般需要以下三个步骤。\n自定义添加步骤\n- 第一步建立一个 ServiceMonitor 对象，用于 Prometheus 添加监控项 - 第二步为 ServiceMonitor 对象关联 metrics 数据接口的一个 Service 对象 - 第三步确保 Service 对象可以正确获取到 metrics 数   举个栗子: 监控etcd集群\n 步骤一：简单定义ServiceMonitor.yaml\n 值得注意的是，这里的serviceMonitor监控方式跟prometheus的原生的方式类似，也支持 metricRelabelings。  apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: etcd-k8s namespace: monitoring labels: k8s-app: etcd-k8s spec: ##1. prometheus收集metrics 的jobname 为labels的value，这里定义为k8s-app,及prometheus job value为etcd-k8s ## 与上面的k8s-app: etcd-k8s对应 jobLabel: k8s-app #2.定义抓取service的方式 endpoints: - port: port interval: 30s scheme: https tlsConfig: caFile: /etc/prometheus/secrets/etcd-certs/ca.crt certFile: /etc/prometheus/secrets/etcd-certs/healthcheck-client.crt keyFile: /etc/prometheus/secrets/etcd-certs/healthcheck-client.key insecureSkipVerify: true #3通过label selector 关联对应的service selector: matchLabels: k8s-app: etcd #4.通过namespaceselector 关联对应的namesapces namespaceSelector: matchNames: - kube-system  步骤二： 定义service.yaml\nservice文件正常定义即可，注意的是service资源的spec.ports.name 一定要跟serviceMonitor资源中的spec.endpoints.port保持一致\napiVersion: v1 kind: Service metadata: name: etcd-k8s namespace: kube-system labels: k8s-app: etcd spec: type: ClusterIP clusterIP: None ports: - name: port port: 2379 protocol: TCP selector: component: etcd  步骤三：保证可抓取\n因为访问etcd需要证书等，因此则需要进行以下操作：\n#1.生成etcd cert证书的secrete cd /etc/kubernetes/pki/etcd/ kubectl create secret generic etcd-certs --from-file=healthcheck-client.crt --from-file=healthcheck-client.key --from-file=ca.crt -n monitoring #2.让service代理的endpoint持有该证书 #在Prometheus-prometheus.yaml 的prometheus资源中添加对应的secretes，它默认会将证书加载到pod中去。 ​``` spec: secrets: - etcd-certs ​``` #3.重新生成prometheus资源。 #3.1 prometheus secrets 挂载到/etc/prometheus/secrets #3.2 /etc/prometheus/config_out/prometheus.env.yaml下会自动生成job_name: monitoring/etcd-k8s/0 的抓取metrics，采用kubernets动态发现的机制抓取 kubectl apply -f prometheus-prometheus   Tips:保证可抓取一般有以下几种情况：\n  需要配置证书 ，https等协议 需要进行clusterrolebing，role操作，对资源对象赋予k8s内可操作权限等  2.2 查看监控项  通过命令的方式  [root@master01 ~]# kubectl get servicemonitor -nmonitoring NAME AGE alertmanager 87d coredns 87d grafana 87d kube-apiserver 87d kube-controller-manager 87d kube-scheduler 87d kube-state-metrics 87d kubelet 87d node-exporter 87d prometheus 87d prometheus-adapter 87d prometheus-operator 87d  通过prometheus Web的方式 如下：   二、Prometheus告警规则 对于Prometheus而言，在原生的管理方式上，我们需要手动创建Prometheus的告警文件，并且通过在Prometheus配置中声明式的加载。而在Prometheus Operator模式中，告警规则也编程一个通过Kubernetes API 声明式创建的一个资源.告警规则创建成功后，通过在Prometheus中使用像servicemonitor那样用**ruleSelector通过label匹配选择需要关联的PrometheusRule即可。**\n2.1 添加告警规则 步骤一： CRD资源prometheus添加ruleselector\nruleSelector: matchLabels: prometheus: k8s role: alert-rules  步骤二： 生成CRD资源prometheusRules，定义规则如下\napiVersion: monitoring.coreos.com/v1 kind: PrometheusRule metadata: labels: app.kubernetes.io/component: prometheus app.kubernetes.io/name: prometheus app.kubernetes.io/part-of: kube-prometheus app.kubernetes.io/version: 2.24.0 ##prometheus-prometheus.yaml中定义的prometheus实例资源通过ruleselector跟prometheusrules建立联系 prometheus: k8s role: alert-rules name: k8s-rules namespace: monitoring spec: ##2.以下类容跟prometheus定义 groups: - name: prometheus rules: - alert: PrometheusBadConfig annotations: description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed to reload its configuration. summary: Failed Prometheus configuration reload. expr: | # Without max_over_time, failed scrapes could create false negatives, see # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details. max_over_time(prometheus_config_last_reload_successful{job=\u0026quot;prometheus-k8s\u0026quot;,namespace=\u0026quot;monitoring\u0026quot;}[5m]) == 0 for: 10m labels: severity: critical  kuber-prometheus operator部署的prometheus集群，会有一个默认的 prometheus告警规则，默认配置文件为kubernetes-prometheusRule.yaml ，定义一个PrometheusRule资源，然后跟定义prometheus默认告警规则一样，大致内容如下：\nprometheus 告警规则配置说明：\n  alert：告警规则的名称。\n  expr：基于PromQL表达式告警触发条件，用于计算是否有时间序列满足该条件。\n  for：评估等待时间，可选参数。用于表示只有当触发条件持续一段时间后才发送告警。在等待期间新产生告警的状态为pending。\n  labels：自定义标签，允许用户指定要附加到告警上的一组附加标签。\n  annotations：用于指定一组附加信息，比如用于描述告警详细信息的文字等，annotations的内容在告警产生时会一同作为参数发送到Alertmanager。\n  通过$labels.\u0026lt;labelname\u0026gt;变量可以访问当前告警实例中指定标签的值。$value则可以获取当前PromQL表达式计算的样本值\n  2.2 查看集群告警规则 1.命令行查看prometheus 规则\n[root@master01 ~]# kubectl get prometheusrules -nmonitoring NAME AGE ingress-nginx-monitor-rules 84d node-monitor-rules 85d openldap-monitor-rules 68d prometheus-k8s-rules 87d pxc-monitor-rules 67d  2.通过web 查看 https://prometheus.k8s.moonton.net/rules\n三、AlertManager 3.1 prometheus添加Alertmanager组件  步骤一：配置crd prometheus资源通过alerting的配置跟altermanager关联，配置如下。  apiVersion: monitoring.coreos.com/v1 kind: Prometheus ... ... spec: alerting: alertmanagers: - name: alertmanager-main namespace: monitoring port: web   步骤二： 配置Alertmanager 资源 ,kube-prometheus项目默认提供如下资源清单，根据需求修改  [root@zsp manifests]# ls alertmanager-* alertmanager-alertmanager.yaml alertmanager-prometheusRule.yaml alertmanager-secret.yaml alertmanager-serviceAccount.yaml alertmanager-serviceMonitor.yaml alertmanager-service.yaml  3.2 Alertmanager 接入钉钉  步骤一： Alertmanager报警组件支持钉钉接入，修改 alertmanager-secret.yaml大致内容如下  apiVersion: v1 kind: Secret metadata: name: alertmanager-main namespace: monitoring stringData: alertmanager.yaml: |- \u0026quot;global\u0026quot;: \u0026quot;resolve_timeout\u0026quot;: \u0026quot;5m\u0026quot; \u0026quot;receivers\u0026quot;: - \u0026quot;name\u0026quot;: \u0026quot;dingtalk-webhook\u0026quot; \u0026quot;webhook_configs\u0026quot;: - \u0026quot;send_resolved\u0026quot;: true \u0026quot;url\u0026quot;: \u0026quot;http://dingtalk-webhook:8060/dingtalk/webhook_mention_all/send\u0026quot; \u0026quot;route\u0026quot;: \u0026quot;group_by\u0026quot;: - \u0026quot;job\u0026quot; \u0026quot;group_interval\u0026quot;: \u0026quot;5m\u0026quot; \u0026quot;group_wait\u0026quot;: \u0026quot;30s\u0026quot; \u0026quot;receiver\u0026quot;: \u0026quot;dingtalk-webhook\u0026quot; \u0026quot;repeat_interval\u0026quot;: \u0026quot;12h\u0026quot; \u0026quot;routes\u0026quot;: - \u0026quot;receiver\u0026quot;: \u0026quot;dingtalk-webhook\u0026quot; \u0026quot;group_wait\u0026quot;: \u0026quot;10s\u0026quot; type: Opaque   步骤二： 运行钉钉webhook  目前钉钉项目路径如下：/usr/local/src/back/config/kube-prometheus/manifests/alertmanager/dingTalk\nalertmanager接入钉钉教程链接\n四、Grafana 集群部署落地的Grafana，也是通过Kube-Prometheus提供的Grafana yaml清单在集群内部署\n4.1 Grafana服务暴露 grafana服务暴露跟kibana一样通过ingress的方式注册到ingress-nginx的网关，从而实现服务暴露，可以通过如下命令查看\n[root@master01 grafana]# kubectl get ingress -nmonitoring access-grafana grafana.k8s.moonton.net 10.142.87.198,10.142.87.209,10.142.87.216 80, 443 95d  值得注意的是，这里也对grafana坐了白名单, 针对OA办公可以访问\nnginx.ingress.kubernetes.io/whitelist-source-range: 180.169.85.114/32,137.59.103.18/32,103.206.188.183/32,101.230.210.194/32,161.202.217.208/32,161.202.217.167/32  4.2 Grafana接入LDAP 可以通过如下命令查看configmap\nkubectl get cm -nmonitoring grafana-config -oyaml  配置ldap配置配置如下：\nldap.toml: | [[servers]] host = \u0026quot;172.31.230.127\u0026quot; port = 389 use_ssl = false start_tls = false ssl_skip_verify = false bind_dn = \u0026quot;cn=admin,dc=moonton,dc=net\u0026quot; bind_password = '2vcsNBhaNHkLW2kVTgBCGs9n' search_filter = \u0026quot;(uid=%s)\u0026quot; search_base_dns = [\u0026quot;ou=People,ou=project-120,dc=moonton,dc=net\u0026quot;] group_search_filter = \u0026quot;(member=uid=%s,ou=People,ou=project-120,dc=moonton,dc=net)\u0026quot; group_search_base_dns = [\u0026quot;ou=Group,ou=project-120,dc=moonton,dc=net\u0026quot;] [servers.attributes] name = \u0026quot;cn\u0026quot; surname = \u0026quot;sn\u0026quot; username = \u0026quot;uid\u0026quot; email = \u0026quot;mail\u0026quot; [[servers.group_mappings]] group_dn = \u0026quot;cn=admin,ou=Group,ou=project-120,dc=moonton,dc=net\u0026quot; org_role = \u0026quot;Admin\u0026quot; grafana_admin = true [[servers.group_mappings]] group_dn = \u0026quot;cn=user,ou=Group,ou=project-120,dc=moonton,dc=net\u0026quot; org_role = \u0026quot;Editor\u0026quot; [[servers.group_mappings]] group_dn = \u0026quot;*\u0026quot; org_role = \u0026quot;Viewer\u0026quot;  五、持久化存储Openebs 目前Prometheus和EleasticSearcher 存储的方式都是基于Openebs的LocalPV的方式进行的数据存储。如何管理Openebs，待补充。。\n","id":3,"section":"posts","summary":"账号管理 Elastic Stack日志方案和Prometheus监控系统的用户权限管理，都是通过接入 openladp同步自研MOA管理系统，统一管理账号，","tags":["kubernetes","operator"],"title":"Prometheus Operator","uri":"https://zhangshunping.github.io/2021/02/prometheus-operator-%E8%90%BD%E5%9C%B0/","year":"2021"},{"content":"Cobra库 cobra 中有个重要的概念，分别是 commands、arguments 和 flags。其中 commands 代表行为，arguments 就是命令行参数(或者称为位置参数)，flags 代表对行为的改变(也就是我们常说的命令行选项)。执行命令行程序时的一般格式为：\n# server是 commands，port 是 flag hugo server --port=1313 # clone 是 commands，URL 是 arguments，brae 是 flag git clone URL --bare  使用  1.安装  go get -u github.com/spf13/cobra/cobra   使用：  #1.初始化，生成程序框架 cd $GOPATH/src ; mkdir -p project cobra init --pkg-name porject #2.添加子命令 command cobra add image cobra add version cobra add container #3.为command添加flag，选项(flags)用来控制 Command 的具体行为。根据选项的作用范围，可以把选项分为两类： #persistent #loca var echoTimes int func init() { rootCmd.AddCommand(imageCmd) cmdTimes.Flags().IntVarP(\u0026amp;echoTimes, \u0026quot;times\u0026quot;, \u0026quot;t\u0026quot;, 1, \u0026quot;times to echo the input\u0026quot;) imageCmd.AddCommand(cmdTimes) }  以package的方式管理子命令 cmd/目录结果如下，要让子命令以package的方式易于管理\n[root@zsp cmd]# pwd /root/gocode/src/hasha/cmd [root@zsp cmd]# tree . ├── mutong │ ├── ml.go │ └── root.go ├── root.go └── version.go 1 directory, 4 files   root.go  var RootCmd = \u0026amp;cobra.Command{ Use: \u0026quot;sp\u0026quot;, Short: \u0026quot;a project to manage your little tools\u0026quot; , Long: ``, // Uncomment the following line if your bare application // has an action associated with it: // Run: func(cmd *cobra.Command, args []string) { }, } // Execute adds all child commands to the root command and sets flags appropriately. // This is called by main.main(). It only needs to happen once to the rootCmd. func Execute() { if err := RootCmd.Execute(); err != nil { fmt.Println(err) os.Exit(1) } } func init() { RootCmd.AddCommand(mutong.MutongCmd) }    mutong/root.go\npackage mutong import ( \u0026quot;github.com/spf13/cobra\u0026quot; ) var echoTime int // testCmd represents the test command var MutongCmd = \u0026amp;cobra.Command{ Use: \u0026quot;mutong\u0026quot;, Short: \u0026quot;some tools in mutong\u0026quot;, Long: ``, } func init() { }    mutong/ml.go\npackage mutong import ( \u0026quot;fmt\u0026quot; \u0026quot;github.com/spf13/cobra\u0026quot; ) // testCmd represents the test command var MlType string var MlCmd = \u0026amp;cobra.Command{ Use: \u0026quot;ml\u0026quot;, Short: \u0026quot;some tools about ml project \u0026quot;, Long: `some tools abount ml project from mutong company`, Run: func(cmd *cobra.Command, args []string) { fmt.Println(\u0026quot;MlCmd called\u0026quot;, echoTime) }, } func init(){ //register to present cmd MutongCmd MutongCmd.AddCommand(MlCmd) MlCmd.Flags().StringP(MlType,\u0026quot;t\u0026quot;,\u0026quot;\u0026quot;,\u0026quot;ml actions\u0026quot;) }    ","id":4,"section":"posts","summary":"Cobra库 cobra 中有个重要的概念，分别是 commands、arguments 和 flags。其中 commands 代表行为，arguments 就是命令行参数(或","tags":["golang"],"title":"Cobra包","uri":"https://zhangshunping.github.io/2021/01/cobra/","year":"2021"},{"content":"client-go client-go 是用 Golang 语言编写的官方编程式交互客户端库，提供对 Kubernetes API server 服务的交互访问。\nclient-go源码目录 1.discovery: 提供 DiscoveryClient 发现客户端。\n2.dynamic: 提供 DynamicClient 动态客户端。\n3.informers: 每种 K8S 资源的 Informer 实现。\n4.kubernetes: 提供 ClientSet 客户端。\n5.listers: 为每一个 K8S 资源提供 Lister 功能，该功能对 Get 和 List 请求提供只读的缓存数据。\n6.plugin: 提供 OpenStack，GCP 和 Azure 等云服务商授权插件。\n7.rest: 提供 RESTClient 客户端，对 K8S API Server 执行 RESTful 操作。\n8.scale: 提供 ScaleClient 客户端，用于扩容或缩容 Deployment, Replicaset, Replication Controller 等资源对象。\n9.tools: 提供常用工具，例如 SharedInformer, Relector, DealtFIFO 及 Indexers。提供 Client 查询和缓存机制，以减少想 kube-apiserver 发起的请求数等。主要子目录为/tools/cache。\n10.transport: 提供安全的 TCP 连接，支持 HTTP Stream，某些操作需要在客户端和容器之间传输二进制流，例如 exec，attach 等操作。该功能由内部的 SPDY 包提供支持。\n11.util: 提供常用方法。例如 WorkQueue 工作队列，Certificate 证书管理等\nclient-go包生成客户端类型 ResetClient是最基础的客户端，其他三个DynamicClient，DicoveryClient, ClientSet客户端都是基于此基础上封装。最常用的客户端ClientSet\n  ClientSet客户端初始化\npackage ExamClient_go import ( \u0026quot;flag\u0026quot; \u0026quot;k8s.io/client-go/kubernetes\u0026quot; kube_rest \u0026quot;k8s.io/client-go/rest\u0026quot; \u0026quot;k8s.io/client-go/tools/clientcmd\u0026quot; \u0026quot;os\u0026quot; \u0026quot;path/filepath\u0026quot; ) // init ClientSet func Connect(env string) (*kubernetes.Clientset, error) { if env == \u0026quot;out-of-cluster\u0026quot; { var kubeconfig *string if home := homeDir(); home != \u0026quot;\u0026quot; { kubeconfig = flag.String(\u0026quot;kubeconfig\u0026quot;, filepath.Join(home, \u0026quot;.kube\u0026quot;, \u0026quot;config\u0026quot;), \u0026quot;(optional) absolute path to the kubeconfig file\u0026quot;) } else { kubeconfig = flag.String(\u0026quot;kubeconfig\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;absolute path to the kubeconfig file\u0026quot;) } config, err := clientcmd.BuildConfigFromFlags(\u0026quot;\u0026quot;, *kubeconfig) if err != nil { panic(err.Error()) } // create the clientset clientset, err := kubernetes.NewForConfig(config) if err != nil { panic(err.Error()) } return clientset, err } else { config, err := kube_rest.InClusterConfig() if err != nil { panic(err.Error()) } // creates the clientset clientset, err := kubernetes.NewForConfig(config) return clientset, err } } func homeDir() string { if h := os.Getenv(\u0026quot;HOME\u0026quot;); h != \u0026quot;\u0026quot; { return h } return os.Getenv(\u0026quot;USERPROFILE\u0026quot;) // windows }     ","id":5,"section":"posts","summary":"client-go client-go 是用 Golang 语言编写的官方编程式交互客户端库，提供对 Kubernetes API server 服务的交互访问。 client-go源码目录 1.discovery: 提供 DiscoveryClient 发现客户端。 2.dynamic: 提供 DynamicClient 动态客户端。 3.informers:","tags":null,"title":"Client-go包","uri":"https://zhangshunping.github.io/2021/01/client-go%E5%B8%B8%E7%94%A8example/","year":"2021"},{"content":"Operator Operator 实际上作为kubernetes自定义扩展资源注册到controller-manager,通过list and watch的方式监听对应资源的变化，然后在周期内的各个环节做相应的协调处理。所谓的处理就是operator实现由状态的应用的核心部分，当然不同应用其处理的方式也不同。\n实现自定一operator一般分为两步：\n1.通过CRD注册对应的资源对象\n2.通过Controller调谐对象的spec和status状态\n一、CRD（Custom Resource Definition） 什么是CRD ​\tkubernetes本身提供了很多apiserver接口，当用户对k8s做operator管理则需要自己自定义一些自定义资源，为了方便扩展kubernetes自定义资源，kubernetes提供了两种方式将Custom resources添加到集群。\n 1.Custom Resource Definitions (CRDs)：更易用、不需要编码。但是缺乏灵活性。 2.API Aggregation：需要编码，允许通过聚合层的方式提供更加定制化的实现。  如何定义CRD 1.普通crd.yaml apiVersion: apiextensions.k8s.io/v1beta1 #使用 apiextensions.k8s.io/v1beta1 API。 kind: CustomResourceDefinition\tmetadata: name: students.test.com ## metadata 要和spec.group对应. 且必须是spec.names.plural 和spec.group组合 spec: group: test.com versions: - name: v1 served: true # 是否有效的开关. storage: true # 只有一个版本能被标注为storage scope: Namespaced # 指定自定义对象可用于某一个项目 (Namespaced) 还是集群中的所有项目 (Cluster)。 names: plural: students # URL中使用的复数名称: /apis/\u0026lt;group\u0026gt;/\u0026lt;version\u0026gt;/\u0026lt;plural\u0026gt; singular: student # 在CLI(shell界面输入的参数)上用作别名并用于显示的单数名称 kind: Student # camelCase 骆驼命名法 # 短名称允许短字符串匹配CLI上的资源，意识就是能通过kubectl 在查看资源的时候使用该资源的简名称来获取。 shortNames: - stu  2. validation 定义需要认证的crd test-validation-crd.yml\napiVersion: apiextensions.k8s.io/v1beta1 #使用 apiextensions.k8s.io/v1beta1 API。 kind: CustomResourceDefinition\tmetadata: name: students.test.com ## metadata 要和spec.group对应. 且必须是spec.names.plural 和spec.group组合 spec: group: test.com versions: - name: v1 served: true # 是否有效的开关. storage: true # 只有一个版本能被标注为storage scope: Namespaced # 指定自定义对象可用于某一个项目 (Namespaced) 还是集群中的所有项目 (Cluster)。 names: plural: students # URL中使用的复数名称: /apis/\u0026lt;group\u0026gt;/\u0026lt;version\u0026gt;/\u0026lt;plural\u0026gt; singular: student # 在CLI(shell界面输入的参数)上用作别名并用于显示的单数名称 kind: Student # camelCase 骆驼命名法 # 短名称允许短字符串匹配CLI上的资源，意识就是能通过kubectl 在查看资源的时候使用该资源的简名称来获取。 shortNames: - stu validation: openAPIV3Schema: description: test properties: spec: properties: age: type: integer minimum: 1 maximum: 10  3.additionalPrinterColumns 添加打印列信息 apiVersion: apiextensions.k8s.io/v1beta1 #使用 apiextensions.k8s.io/v1beta1 API。 kind: CustomResourceDefinition metadata: name: students.test.com ## metadata 要和spec.group对应. 且必须是spec.names.plural 和spec.group组合 spec: group: test.com versions: - name: v1 served: true # 是否有效的开关. storage: true # 只有一个版本能被标注为storage scope: Namespaced # 指定自定义对象可用于某一个项目 (Namespaced) 还是集群中的所有项目 (Cluster)。 names: plural: students # URL中使用的复数名称: /apis/\u0026lt;group\u0026gt;/\u0026lt;version\u0026gt;/\u0026lt;plural\u0026gt; singular: student # 在CLI(shell界面输入的参数)上用作别名并用于显示的单数名称 kind: Student # camelCase 骆驼命名法 # 短名称允许短字符串匹配CLI上的资源，意识就是能通过kubectl 在查看资源的时候使用该资源的简名称来获取。 shortNames: - stu validation: openAPIV3Schema: description: test properties: spec: properties: age: type: integer minimum: 1 maximum: 20 additionalPrinterColumns: - name: School type: string JSONPath: .spec.school - name: Age type: date JSONPath: .metadata.creationTimestamp  [root@zsp workload]# kubectl get stu NAME SCHOOL AGE beijing 清华 3m12s  etcdctl查看注册的 crd资源\nalias k8s-etcdctl=\u0026quot;ETCDCTL_API=3 etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/peer.crt --key=/etc/kubernetes/pki/etcd/peer.key \u0026quot; k8s-etcdctl get /registry/apiextensions.k8s.io/customresourcedefinitions/students.test.com --prefix  创建CR资源 stu-cr.yaml\napiVersion: test.com/v1 kind: Student metadata: name: beijing spec: name: \u0026quot;张三\u0026quot; school: \u0026quot;清华\u0026quot; age: 19   验证普通crd  [root@zsp workload]# kubectl apply -g stu-cr.yaml [root@zsp workload]# kubectl get stu beijing -oyaml apiVersion: test.com/v1 kind: Student metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\u0026quot;apiVersion\u0026quot;:\u0026quot;test.com/v1\u0026quot;,\u0026quot;kind\u0026quot;:\u0026quot;Student\u0026quot;,\u0026quot;metadata\u0026quot;:{\u0026quot;annotations\u0026quot;:{},\u0026quot;name\u0026quot;:\u0026quot;beijing\u0026quot;,\u0026quot;namespace\u0026quot;:\u0026quot;default\u0026quot;},\u0026quot;spec\u0026quot;:{\u0026quot;age\u0026quot;:19,\u0026quot;name\u0026quot;:\u0026quot;张三\u0026quot;,\u0026quot;school\u0026quot;:\u0026quot;清华\u0026quot;}} creationTimestamp: \u0026quot;2021-01-11T07:30:55Z\u0026quot; generation: 2 name: beijing namespace: default resourceVersion: \u0026quot;2046925\u0026quot; selfLink: /apis/test.com/v1/namespaces/default/students/beijing uid: a47568b7-305e-406c-84e2-114e61db80f3 spec: age: 19 name: 张三 school: 清华  二、自定义控制器Controller 编写自定义控制器分三步  第一步： 初始化（按照需求创建一个 类似Kubernetes 的 client（kubeClient））或者 其他初始化动作。 第二步：main 函数为自定义对象（如自定义NetWork对象）创建一个叫作 InformerFactory（即：networkInformerFactory）的工厂，并使用它生成一个 Network 对象的 Informer，传递给控制器Controller。 第三步：main 函数启动上述的 Informer，然后执行 controller.Run，启动自定义控制器。  func main() { ... cfg, err := clientcmd.BuildConfigFromFlags(masterURL, kubeconfig) ... kubeClient, err := kubernetes.NewForConfig(cfg) ... networkClient, err := clientset.NewForConfig(cfg) ... //2.初始化infomerFactory 工厂 networkInformerFactory := informers.NewSharedInformerFactory(networkClient, ...) // 将自定义对象的工厂 注册到控制器中去 controller := NewController(kubeClient, networkClient, networkInformerFactory.Samplecrd().V1().Networks()) // 启动informer go networkInformerFactory.Start(stopCh) // 启动控制器 if err = controller.Run(2, stopCh); err != nil { glog.Fatalf(\u0026quot;Error running controller: %s\u0026quot;, err.Error()) } }  自定控制器流程 到此，自定义控制器流程完成，是否疑惑，就这么简单？\n别急 接下来，我就为你详细解释一下这个自定义控制器的工作原理，如图：\n从左往右看：\u0026mdash;-\u0026gt;\n首先,控制器中的informer如何工作的呢？，控制器需要从kubeapiserver中去获取他关心的对象，这种操作依靠了informer机制，informer中Reflector包使用了一种list/watch的方式，来“list”并“watch” 关心的对象变化。在ListAndWatch工作模式下，关注对象（这里假设为NetWork自定义对象），一旦apiserver端有新的Network创建，更新，删除，Reflector都会收到 \u0026ldquo;事件通知\u0026rdquo;。这时，该 事件和对应的api对象的组合 被称为增量（Delta），它会被放进一个 Delta FIFO Queue（即：增量先进先出队列）中。\n而另一方面，Informe 会不断地从这个 Delta FIFO Queue 里读取（Pop）增量。每拿到一个增量，**Informer 就会判断这个增量里的事件类型，然后创建或者更新本地对象的缓存。**这个缓存，在 Kubernetes 里一般被叫作 Store。比如，如果事件类型是 Added（添加对象），那么 Informer 就会通过一个叫作 Indexer 的库把这个增量里的 API 对象保存在本地缓存中，并为它创建索引。相反，如果增量的事件类型是 Deleted（删除对象），那么 Informer 就会从本地缓存中删除这个对象。\n这个同步本地缓存的工作，是 Informer 的第一个职责，也是它最重要的职责。而 Informer 的第二个职责，则是根据这些事件的类型，触发事先注册好的 ResourceEventHandler. 这些 Handler，需要在创建控制器的时候注册给它对应的 Informer，通过对应的事件，将事件对应的api对象加入到workqueue中去，需要注意的是，实际入队的并不是 API 对象本身，而是它们的 Key，即：该 API 对象的/。\n而我们编写的控制循环，会不断的从workqueue中拿到这些key，然后开始真正的调谐逻辑。\nInformer总结：综合上面的讲述，你现在应该就能明白**，所谓 Informer，其实就是一个带有本地缓存和索引机制的、可以注册 EventHandler 的 client。它是自定义控制器跟 APIServer 进行数据同步的重要组件**。他分别做了:\n 1.list/watch 获取最新api 对象，然后watch这些最新的对象的变化。 2.通过监听对象变化，infomer可以实时的更新本地缓存，infomer同时注册了三个事件，份别对应 API 对象的“添加”“更新”和“删除”事件。而具体的处理操作，都是将该事件对应的 API 对象加入到工作队列（workqueue）中。  注意的是：\tinformer的本地存储，每经过 resyncPeriod 指定的时间，Informer 维护的本地缓存，都会使用最近一次 LIST 返回的结果强制更新一次，从而保证缓存的有效性。在 Kubernetes 中，这个缓存强制更新的操作就叫作：resync。 这个定时 resync 操作，也会触发 Informer 注册的“更新”事件，因此判断对象的新旧，先判断了一下新、旧两个 Network 对象的版本（ResourceVersion）是否发生了变化，然后才开始进行的入队操作。\nfunc NewController( kubeclientset kubernetes.Interface, networkclientset clientset.Interface, networkInformer informers.NetworkInformer) *Controller { ... controller := \u0026amp;Controller{ kubeclientset: kubeclientset, networkclientset: networkclientset, networksLister: networkInformer.Lister(), networksSynced: networkInformer.Informer().HasSynced, //初始化工作队列 workqueue: workqueue.NewNamedRateLimitingQueue(..., \u0026quot;Networks\u0026quot;), ... } //Informer 通知注册事件AddEventHandler networkInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ AddFunc: controller.enqueueNetwork, UpdateFunc: func(old, new interface{}) { oldNetwork := old.(*samplecrdv1.Network) newNetwork := new.(*samplecrdv1.Network) //update事件，需要判断ResourceVersion if oldNetwork.ResourceVersion == newNetwork.ResourceVersion { return } controller.enqueueNetwork(new) }, DeleteFunc: controller.enqueueNetworkForDelete, return controller }  然后，控制器Control Loop 是如何工作的呢？伪代码如下：\nfunc (c *Controller) Run(threadiness int, stopCh \u0026lt;-chan struct{}) error { ... if ok := cache.WaitForCacheSync(stopCh, c.networksSynced); !ok { return fmt.Errorf(\u0026quot;failed to wait for caches to sync\u0026quot;) } ... for i := 0; i \u0026lt; threadiness; i++ { go wait.Until(c.runWorker, time.Second, stopCh) } ... return nil }  其实很简单\n 首先，等待 Informer 完成一次本地缓存的数据同步操作； 然后，通过goroutine启动“无限循环”任务  那么“无限循环任务”是如何做的呢？\n  首先从WorkQueue中Get一个成员，及一个对象的key。\n  然后拿着这个key，尝试从从Informer本地缓存中去拿到这个key对应的对象，获取对象的时候会发生两种可能\n  1.当这个对象的isNotFound的时候，说明这个对象的key ，之前有\u0026quot;delete\u0026quot;事件发生，本地缓存对应的对象已经被删除了，所以没有办法获取到，而发生“delete”事件时候，WorkQueue中添加了这次“delete”事件的key。\n  2.如果能够获取到对应的 Network 对象，我就可以执行控制器模式里的对比“期望状态”和“实际状态”的逻辑了 及Reconciler 调谐程序，也就是使用类似operator-sdk这样进行operator开发，大部分代码工作的部分\n    Controller伪代码如下：\nfunc (c *Controller) runWorker() { for c.processNextWorkItem() { } } func (c *Controller) processNextWorkItem() bool { obj, shutdown := c.workqueue.Get() ... err := func(obj interface{}) error { ... if err := c.syncHandler(key); err != nil { return fmt.Errorf(\u0026quot;error syncing '%s': %s\u0026quot;, key, err.Error()) } c.workqueue.Forget(obj) ... return nil }(obj) ... return true } func (c *Controller) syncHandler(key string) error { //从infomrer的local store中获取 namespace, name, err := cache.SplitMetaNamespaceKey(key) ... network, err := c.networksLister.Networks(namespace).Get(name) if err != nil { // 如果没有则证明，已经被delete if errors.IsNotFound(err) { glog.Warningf(\u0026quot;Network does not exist in local cache: %s/%s, will delete it from Neutron ...\u0026quot;, namespace, name) return nil } ... return err } glog.Infof(\u0026quot;[Neutron] Try to process network: %#v ...\u0026quot;, network) return nil }  实际上，在 Kubernetes 的源码中，你会经常看到控制器从各种 Lister 里获取对象，比如：podLister、nodeLister 等等，它们使用的都是 Informer 和缓存机制。\n三、思考 1.CRD在ETCD里面是以JSON格式来存储的，而K8S的API对象是以protobuf格式存储，在资源对象数量多的时候JSON的序列化和反序列化性能会成为瓶颈\n2.controller 中使用workqueue的方式来解耦，因为list/watch的工作跟从controller loop的动作有快有慢，面对这样的场景，使用列队的方式，可以有效的防止阻塞。\n","id":6,"section":"posts","summary":"Operator Operator 实际上作为kubernetes自定义扩展资源注册到controller-manager,通过list and watch的方式监听对应资源的变化","tags":["operator"],"title":"Operator(一)","uri":"https://zhangshunping.github.io/2021/01/operator-1/","year":"2021"},{"content":"[toc]\nkubernetes创建过程 全局周期 k8s创建pod的组件之间的交互过程，看图说话:\n  步骤一：用户向apiserver提交创建pod请求，kube-apiserver接受到请求之后会做以下\n 1.1根据提交的数据创建一个pod资源对象 1.2对比元素据和上下文的namespace是否一致，如果不匹配则创建失败 1.3ns匹配之后，在注入一些系统数据，如：podname未指定，则将pod的uid作为podname 1.4 apiserver检验创建的pod对象的必须字段是否为空，如果为空，则创建失败 上述完成之后，将pod资源持久化到etcd中，异步调用返回结果为restful.reponse,完成结果反馈    步骤二：scheduler 采用informer机制list/watch api-server，发现有新pod资源\n  2.1 判断pod.spec.name=null，如果为true 则证明是未被分配的pod,则会进项调度策略，分为预选策略，节点优选\n  2.2 节点预选：基于一系列预选规则（如 PodFitsResource 和 MatchNode-Selector 等）对每个节点进行检查，将不符合的节点过滤掉从而完成节点预选。\n  2.3 节点优选：对预选出的节点进行优先级排序，以便选出最适合运行 pod 对象的节点。\n  2.4 从优先级结果中挑选出优先级最高的节点来运行 pod 对象，当此类节点多个时则随机选择一个。\n  将bound信息通过apiserver，持久化到etcd服务中去\n    步骤三：kubelet 同样采用informer机制list/watch apiserver\n 3.1当新的node（及kubescheduler绑定的node）需要被调度pod，同时这个node的编号跟自己的注册编号一致，则会调用csi接口创建存储，调用runtime（CRI和CNI船舰对应的网络和runtime）。 3.2 将pod信息更新返回给apiserver，apiserver将信息持久化到etcd中去    kubelet创建pod周期过程 引用 ： 图解kubernetes Pod创建流程大揭秘\n","id":7,"section":"posts","summary":"[toc] kubernetes创建过程 全局周期 k8s创建pod的组件之间的交互过程，看图说话: 步骤一：用户向apiserver提交创建pod请求，k","tags":["kubernetes"],"title":"kubernetes创建pod周期一览","uri":"https://zhangshunping.github.io/2020/11/k8s-period/","year":"2020"},{"content":"[toc]\nkubernetes创建过程 全局周期 k8s创建pod的组件之间的交互过程，看图说话:\n  步骤一：用户向apiserver提交创建pod请求，kube-apiserver接受到请求之后会做以下\n 1.1根据提交的数据创建一个pod资源对象 1.2对比元素据和上下文的namespace是否一致，如果不匹配则创建失败 1.3ns匹配之后，在注入一些系统数据，如：podname未指定，则将pod的uid作为podname 1.4 apiserver检验创建的pod对象的必须字段是否为空，如果为空，则创建失败 上述完成之后，将pod资源持久化到etcd中，异步调用返回结果为restful.reponse,完成结果反馈    步骤二：scheduler 采用informer机制list/watch api-server，发现有新pod资源\n  2.1 判断pod.spec.name=null，如果为true 则证明是未被分配的pod,则会进项调度策略，分为预选策略，节点优选\n  2.2 节点预选：基于一系列预选规则（如 PodFitsResource 和 MatchNode-Selector 等）对每个节点进行检查，将不符合的节点过滤掉从而完成节点预选。\n  2.3 节点优选：对预选出的节点进行优先级排序，以便选出最适合运行 pod 对象的节点。\n  2.4 从优先级结果中挑选出优先级最高的节点来运行 pod 对象，当此类节点多个时则随机选择一个。\n  将bound信息通过apiserver，持久化到etcd服务中去\n    步骤三：kubelet 同样采用informer机制list/watch apiserver\n 3.1当新的node（及kubescheduler绑定的node）需要被调度pod，同时这个node的编号跟自己的注册编号一致，则会调用csi接口创建存储，调用runtime（CRI和CNI船舰对应的网络和runtime）。 3.2 将pod信息更新返回给apiserver，apiserver将信息持久化到etcd中去    kubelet创建pod周期过程 引用 ： 图解kubernetes Pod创建流程大揭秘\n","id":8,"section":"posts","summary":"[toc] kubernetes创建过程 全局周期 k8s创建pod的组件之间的交互过程，看图说话: 步骤一：用户向apiserver提交创建pod请求，k","tags":["kubernetes"],"title":"kubernetes创建pod周期一览","uri":"https://zhangshunping.github.io/2020/11/k8s%E5%88%9B%E5%BB%BApod%E5%91%A8%E6%9C%9F/","year":"2020"},{"content":"[TOC]\n监控维度 怎么监控？\n 硬件  温度 磁盘，主板等故障 待机时间   系统  cpu 内存 负载 磁盘 内核参数 网卡流量 tcp连接数 进程数 io 端口采集   程序  应用： nginx，mysql等 业务：收入，人数，活跃ip，净增长数等 业务流量： uv，pv等流量指标 日志： 错误日志，程序日志，访问日志 业务接口： api接口信息，接口相应时间    zabbix介绍 zabbix监控模式 zabbix 跟prometheus 一样，分为主动模式和被动模式\n 主动模式：对标prometheus的push的方式，agent端主动向server端推送数据 被动模式：对标prometheus的node-exporter机制，server端主动采集agent的数据  zabbix架构 docker安装部署zabbix  安装前配置  #一、配置CentOS 7 #关闭selinux #setenforce 0 临时关闭 #需要关闭 selinux，一定要关闭这个，开启selinux会引起一连串问题，甚至zabbix的discovery功能也不能正常使用 sed -i\u0026quot;s/SELINUX=enforcing/SELINUX=disabled/g\u0026quot; /etc/selinux/config #确认是否修改成功 grep SELINUX/etc/selinux/config #然后重启系统即可 #关闭防火墙，#启动firewall，停止firewall，停止firewall systemctl startfirewalld.service systemctl stopfirewalld.service systemctl disablefirewalld.service   快速安装  docker network create zabbix-net #1.安装zabbix-server docker pull zabbix/zabbix-appliance:latest # 拉取镜像 docker run --name zabbix-server -t \\ -p 10051:10051 \\ -p 80:80 \\ --net zabbix-net -d zabbix/zabbix-appliance:latest #2.安装zabbix-agent docker run -dit --name zabbix-agent --net zabbix-net centos:7  配置监控 配置zabbix_agent  配置zabbix_agent.conf 配置  PidFile=/var/run/zabbix/zabbix_agentd.pid LogFile=/var/log/zabbix/zabbix_agentd.log LogFileSize=0 SourceIP=127.0.0.1 Server=172.16.94.152,172.16.82.83 ListenPort=10050 Hostname=172.16.95.1 Include=/etc/zabbix/zabbix_agentd.d/*.conf AllowRoot=1 // 指定用户，如果非root用户，要考虑用户的权限，一般设置sudo nopass User=root // 接受远程命令 EnableRemoteCommands=1 //执行远程命令日志 LogRemoteCommands=1 UnsafeUserParameters=1#是否允许自定义脚本传递特殊字符作为参数,通常不用开 UserParameter=crondstatus,/usr/local/etc/chkcrond.sh crondstatus UserParameter=chkdisk,/usr/local/etc/chkdisk.sh chkdisk UserParameter=sys10,/usr/local/etc/sys10.sh sys10 UserParameter=tcp.status[*],/usr/local/etc/tcp_connection_status.sh $1 UserParameter=chkthread,/usr/local/etc/chkthread.sh chktread UserParameter=chkpro,/usr/local/etc/chkpro.sh chkpro UserParameter=pod.status[*],/usr/local/etc/chkpodstatus.sh   重启zabbix 脚本。restart_zabbix.sh  #!/bin/bash pfurl='/usr/sbin/zabbix_agentd -c /etc/zabbix/zabbix_agentd.conf' PID=0 kill_zabbix_agentd(){ ps aux | awk -r '/zabbix_agentd/{print $2}' | xargs -n1 -exec kill {} \u0026amp;\u0026gt; /dev/null } pfstart(){ num=`ps aux | awk -r '/zabbix_agentd/{print $2}' | wc -l` if [ $num != \u0026quot;1\u0026quot; ];then echo \u0026quot;zabbix_agentd is useing\u0026quot; else echo \u0026quot;zabbix_agentd is starting...\u0026quot; $pfurl echo \u0026quot;start done!\u0026quot; fi } if [ \u0026quot;$1\u0026quot; == \u0026quot;-s\u0026quot; ];then if [ \u0026quot;$2\u0026quot; == \u0026quot;stop\u0026quot; ];then echo \u0026quot;zabbix_agentd is stopping....\u0026quot; kill_zabbix_agentd echo \u0026quot;stop done!\u0026quot; elif [ \u0026quot;$2\u0026quot; == \u0026quot;start\u0026quot; ];then pfstart elif [ \u0026quot;$2\u0026quot; == \u0026quot;restart\u0026quot; ];then echo \u0026quot;zabbix_agentd is stopping....\u0026quot; kill_zabbix_agentd echo \u0026quot;stop done!\u0026quot; echo \u0026quot;zabbix_agentd is starting...\u0026quot; $pfurl echo \u0026quot;start done!\u0026quot; fi elif [ \u0026quot;$1\u0026quot; == \u0026quot;--help\u0026quot; ];then echo \u0026quot;zabbix_agentd -s [start/stop/restart]\u0026quot; else pfstart fi  zabbix的监控项 写在前面 ​\t常见的监控项说明\n​\thttps://blog.csdn.net/u011731378/article/details/80154393\n​ https://www.zabbix.com/documentation/2.2/manual/config/items/itemtypes/zabbix_agent\n监控项存储的值   As is：不对数据处理的原始值\n  Delta：（变化）本次采集减去前一次采集的结果，（速率），本次减去前一次采样的值，再除以经过的时长，类似与prometheus的rate\n常见监控项目 监控写法\n  删除监控项  操作步骤 1.清除历史（如有必要）2.删除  zabbix 中文乱码问题 ​\t解决链接：\thttps://blog.csdn.net/Callousmaster/article/details/99972089\nzabbix触发器 zabbix模板 针对一个主机item，触发器，动作等可以定义到模板里，通过管理主机（主机组）和模板，可以快速方便添加\nzabbix 宏 全局宏，模板宏，主机宏（类似全局变量，私有变量的概念，反正都是花里胡哨的）\n对应的宏的链接地址：\nhttps://www.zabbix.com/documentation/4.0/zh/manual/appendix/macros/supported_by_location\n自定义监控 格式：\nUserParameter=key，command\n例如：\nUserParameter=crondstatus,/usr/local/etc/chkcrond.sh crondstatus\n//command 加入位置参数，需要加上这个选项 UnsafeUserParameters=1 ## key 为crontstats， command 为/use/local/etc/chckcront.sh crontastatus UserParameter=crondstatus,/usr/local/etc/chkcrond.sh crondstatus ##跟上面一样，不过key 值为 pod.status[$1], value为/usr/local/etc/chkpodstatus.sh $1， ##这里的$1及为pod.status中的*的传参 UserParameter=pod.status[*],/usr/local/etc/chkpodstatus.sh $1  【自定义监控内存操作步骤】    自定义采集 脚本，temtest.sh 脚本 ，赋执行权限    #!/bin/bash s=$1 getMem(){ cat /proc/meminfo |awk '/^'$s'/ {print $2}' } getMem    添加 /etc/zabbix/zabbix_agent.conf配置  UnsafeUserParameters=1 Parameter=mem.own[*],/etc/zabbix/testmem.sh $1     重启zabbix-agent ,在server端执行\nzabbix_get -s sourceip -p 10050 -k 'mem.own[MemFree]'  检测是否收集到数据。\n     zabbix web界面添加监控下如下：\n    zabbix 网络发现 zabbix的自动发现机制：一般情况下，发现机制原理如下吗，server端配置discovery机制，发现同一个ip内的zabbix-agent的端口，给他关联上对应动作，比如关联监控模板，从而一个新机器可以直接上报监控模板的数据到server端。\n网络发现配置步骤：\n 1、安装zabbix-agent并设置zabbix_agent.conf的配置 2、在zabbix server配置自动发现的策略，和对应的动作  引用 zabbix网络发现教程\n","id":9,"section":"posts","summary":"[TOC] 监控维度 怎么监控？ 硬件 温度 磁盘，主板等故障 待机时间 系统 cpu 内存 负载 磁盘 内核参数 网卡流量 tcp连接数 进程数 io 端口采集 程序 应用： nginx，my","tags":["zabbix","监控"],"title":"Zabbix(一)","uri":"https://zhangshunping.github.io/2020/11/zabbix-1/","year":"2020"},{"content":"[toc]\ndynamic-Scheduler 背景 kubernetes调度策略默认根据node节点的request值（cpu和mem）进行调度，dynamic-Scheduler希望能够解决实际情况过程中遇到如下痛点\n  request设置太小，导致node节点上被分配太多的pod，面对流量高峰时，当前节点的资源使用率过高，导致一些pod 出现oom或者无法执行业务的情况。\n  request接近于limits,导致node节点上分配的pod数量有限，不能够最大容量的分配pod，而本身设置limits值的原因大多是基于高峰资源，而实际业务大多数情况并未处于高峰状态，所以会造成资源浪费。\n  个性化原因：\n  我们的业务模型创建的pod是自主性pod，虽然社区针对这样的问题有一些好用的开源的平衡组件descheduler，但不满足实际的问题，我们需要的是一个在调度的时候就可以根据资源消耗情况选择调度的组件，而descheduler是通过controller的方式对正在集群内的pod进行平衡，不满足我们目前的需求。\n  kubelet内存驱逐条件（memory.available）,是通过计算cgroups下资源换算得来的，我们的业务是频繁的创建pod环境，然后销毁，可能会有一些pod跑业务的gc出现问题，导致内存无法释放，而此时free -h看到的内存可使用率仍然ok，但实际上/sys/fs/cgroups/memory下的memory.usage_in_bytes的值已经接近整个内存总量了。根据公式\nmemory.available := node.status.capacity[memory]-node.stats.memory.workingSet  此时memory.available已经低于节点驱逐的available值，就会出现pod被驱逐。因此对于cgroups下memory.available的值也需要做一定的监控和调度策略，kubectl top 命令计算好像不太准确，因此使用pushGateWay的方式，收集节点的memory.available值到prometheus。根据kubelet计算memory.available脚本进行push。\n    解决思路 通过监控获取到各个k8s节点的使用率（cpu，mem等），当超过阈值后，给对应节点打上status=presure，根据k8s软亲和性策略，让pod尽量不调度到打上的presure标签的节点。\n  prometheus监控k8s集群的node资源使用率（因为是云原生背景，所以要自动发现弹性节点node，并对弹性节点进行监控）。需注意\n prometheus.yaml文件中采集node资源时，需要用如下的动态relabel机制，因为，client_golang采集Prometheus的metrics的数据为string，在dynamic-scheduler中使用ConvertResultDataType函数将string转换成[]string格式,这里的values为instance的值。 需要安装pushgateway，然后crontab的方式运行push.sh脚本，进行memory.available的采集   - job_name: '测试环境k8s资源节点监控' kubernetes_sd_configs: - role: node relabel_configs: - source_labels: [__meta_kubernetes_node_address_InternalIP] regex: '(.*)' replacement: '${1}:9100' target_label: __address__ - job_name: 'pushgateway' honor_labels: true static_configs: - targets: ['172.16.95.4:49091']    判断prometheus的返回得node使用率的metrics是否超过阈值，从而给k8s node节点打标签status=presure,根据k8s default的调度策略的软亲和preferredDuringSchedulingIgnoredDuringExecution策略，调度的时候尽量不调度到被打上标签status=presure的节点及当前资源紧张的节点。\ntemplate: metadata: labels: run: nginx spec: affinity: nodeAffinity: preferredDuringSchedulingIgnoredDuringExecution: - preference: matchExpressions: - key: status operator: NotIn values: - presure weight: 20    考虑到apiserver的压力问题，使用client—go 原生informer机制，减少对apiserver的压力。\n  promethues和dynamic-Scheduler 挂了情况，也不能影响到实际业务，因为使用的软亲和的方式，所以最终一定会有node被调度\n  使用方式 ./dynamic-sheduler --help -cmem float kubelet驱逐memory.availabele的阈值 (-cmem 2040) (default 2040) -cpu float 节点过去一分钟使用率阈值 (-cpu 10) (default 60) -kubeconfig string 链接k8s kubeconfig的绝对路径 (default \u0026quot;C:\\\\Users\\\\39295\\\\go\\\\src\\\\config\u0026quot;) -mem float 节点内存使用率阈值 (-mem 10) (default 80) -prom string prometheus链接地址(-prom http://121.40.XX.XX:49090) (default \u0026quot;http://121.40.XX.XX:49090\u0026quot;) -promjob string promtheus采集的node节点job名称(-promjob 测试环境k8s资源节点监控) (default \u0026quot;测试环境k8s资源节点监控\u0026quot;) -s int 每次抓取prometheus metrics间隔（-s 10) (default 10) -webaddr string 启动服务端口地址(-webaddr :9000) (default \u0026quot;:9000\u0026quot;) exit status 2   健康检测  curl ip:port/status  返回字符串 \u0026ldquo;ok\u0026rdquo;\n 查询多少个节点打上了type=presure标签（返回json)  curl ip:port/nodes  返回json\n { \u0026quot;type\u0026quot;:\u0026quot;Label type=presure Nodes\u0026quot;,\u0026quot;num\u0026quot;:2,\u0026quot;node_names\u0026quot;:[\u0026quot;host1\u0026quot;,\u0026quot;hosts2\u0026quot;]}\n","id":10,"section":"posts","summary":"[toc] dynamic-Scheduler 背景 kubernetes调度策略默认根据node节点的request值（cpu和mem）进行调度，dynamic-Scheduler希望","tags":["Golang","kubernetes"],"title":"开源小工具- Dynamic Scheduler","uri":"https://zhangshunping.github.io/2020/09/dynamic-scheduler/","year":"2020"},{"content":"[toc]\n 监控的目的 在《SRE: Google运维解密》一书中指出，监控系统需要能够有效的支持白盒监控和黑盒监控。通过白盒能够了解其内部的实际运行状态，通过对监控指标的观察能够预判可能出现的问题，从而对潜在的不确定因素进行优化。而黑盒监控，常见的如HTTP探针，TCP探针等，可以在系统或者服务在发生故障时能够快速通知相关的人员进行处理。通过建立完善的监控体系，从而达到以下目的：\n 长期趋势分析：通过对监控样本数据的持续收集和统计，对监控指标进行长期趋势分析。例如，通过对磁盘空间增长率的判断，我们可以提前预测在未来什么时间节点上需要对资源进行扩容。 对照分析：两个版本的系统运行资源使用情况的差异如何？在不同容量情况下系统的并发和负载变化如何？通过监控能够方便的对系统进行跟踪和比较。 告警：当系统出现或者即将出现故障时，监控系统需要迅速反应并通知管理员，从而能够对问题进行快速的处理或者提前预防问题的发生，避免出现对业务的影响。 故障分析与定位：当问题发生后，需要对问题进行调查和处理。通过对不同监控监控以及历史数据的分析，能够找到并解决根源问题。 数据可视化：通过可视化仪表盘能够直接获取系统的运行状态、资源使用情况、以及服务运行状态等直观的信息。   Promql 什么是时间序列 通过Node Exporter暴露的HTTP服务，Prometheus可以采集到当前主机所有监控指标的样本数据。例如：\n# HELP node_cpu Seconds the cpus spent in each mode. # TYPE node_cpu counter node_cpu{cpu=\u0026quot;cpu0\u0026quot;,mode=\u0026quot;idle\u0026quot;} 362812.7890625 # HELP node_load1 1m load average. # TYPE node_load1 gauge node_load1 3.0703125  其中非#开头的每一行表示当前Node Exporter采集到的一个监控样本：node_cpu和node_load1表明了当前指标的名称、大括号中的标签则反映了当前样本的一些特征和维度、浮点数则是该监控样本的具体值。\n样本 Prometheus将所有的样本数据以时间序列(time-series)的方式存放在内存里，并定时保存到硬盘上。\ntime-series是按照时间戳和值的序列顺序存放的，我们称之为向量(vector). 每条time-series通过指标名称(metrics name)和一组标签集(labelset)命名。如下所示，可以将time-series理解为一个以时间为Y轴的数字矩阵：\n ^ │ . . . . . . . . . . . . . . . . . . . node_cpu{cpu=\u0026quot;cpu0\u0026quot;,mode=\u0026quot;idle\u0026quot;} │ . . . . . . . . . . . . . . . . . . . node_cpu{cpu=\u0026quot;cpu0\u0026quot;,mode=\u0026quot;system\u0026quot;} │ . . . . . . . . . . . . . . . . . . node_load1{} │ . . . . . . . . . . . . . . . . . . v \u0026lt;------------------ 时间 ----------------\u0026gt;  在time-series中的每一个点称为一个样本（sample），样本由以下三部分组成：\n 指标(metric)：metric name和描述当前样本特征的labelsets; 时间戳(timestamp)：一个精确到毫秒的时间戳; 样本值(value)： 一个float64的浮点型数据表示当前样本的值。  \u0026lt;--------------- metric ---------------------\u0026gt;\u0026lt;-timestamp -\u0026gt;\u0026lt;-value-\u0026gt; http_request_total{status=\u0026quot;200\u0026quot;, method=\u0026quot;GET\u0026quot;}@1434417560938 =\u0026gt; 94355 http_request_total{status=\u0026quot;200\u0026quot;, method=\u0026quot;GET\u0026quot;}@1434417561287 =\u0026gt; 94334 http_request_total{status=\u0026quot;404\u0026quot;, method=\u0026quot;GET\u0026quot;}@1434417560938 =\u0026gt; 38473 http_request_total{status=\u0026quot;404\u0026quot;, method=\u0026quot;GET\u0026quot;}@1434417561287 =\u0026gt; 38544 http_request_total{status=\u0026quot;200\u0026quot;, method=\u0026quot;POST\u0026quot;}@1434417560938 =\u0026gt; 4748 http_request_total{status=\u0026quot;200\u0026quot;, method=\u0026quot;POST\u0026quot;}@1434417561287 =\u0026gt; 4785  指标(Metric) 在形式上，所有的指标(Metric)都通过如下格式标示：\n\u0026lt;metric name\u0026gt;{\u0026lt;label name\u0026gt;=\u0026lt;label value\u0026gt;, ...}  指标的名称(metric name)可以反映被监控样本的含义（比如，http_request_total - 表示当前系统接收到的HTTP请求总量）。指标名称只能由ASCII字符、数字、下划线以及冒号组成并必须符合正则表达式[a-zA-Z_:][a-zA-Z0-9_:]*。\n标签(label)反映了当前样本的特征维度，通过这些维度Prometheus可以对样本数据进行过滤，聚合等。标签的名称只能由ASCII字符、数字以及下划线组成并满足正则表达式[a-zA-Z_][a-zA-Z0-9_]*。\n其中以__作为前缀的标签，是系统保留的关键字，只能在系统内部使用。标签的值则可以包含任何Unicode编码的字符。在Prometheus的底层实现中指标名称实际上是以__name__=的形式保存在数据库中的，因此以下两种方式均表示的同一条time-series：\napi_http_requests_total{method=\u0026quot;POST\u0026quot;, handler=\u0026quot;/messages\u0026quot;}  等同于：\n{__name__=\u0026quot;api_http_requests_total\u0026quot;，method=\u0026quot;POST\u0026quot;, handler=\u0026quot;/messages\u0026quot;}  在Prometheus源码中也可以找到指标(Metric)对应的数据结构，如下所示：\ntype Metric LabelSet type LabelSet map[LabelName]LabelValue type LabelName string type LabelValue string  数据类型 Counter:只增不减的计数器  一般在定义Counter类型指标的名称时推荐使用_total作为后缀,如http_requests_total，node_cpu都是Counter类型的监控指标  Gauge: 可增可减的仪表盘  Gauge类型的指标侧重于反应系统的当前状态。因此这类指标的样本数据可增可减。常见指标如：node_memory_MemFree（主机当前空闲的内容大小）、node_memory_MemAvailable（可用内存大小）都是Gauge类型的监控指标。通过Gauge指标，用户可以直接查看系统的当前状态node_memory_MemFree  Summary \u0026amp;\u0026amp; Histogram   适用于一般使用于解决长尾问题\n  Histogram 统计数据的分部情况。比如最小值，最大值，中间值，中位数，75百分位，90百分位，95/98/99/99.9百分位的值（percentlies）\n这是一种特殊的 metrics 数据类型，代表的是一种 近似的百分比估算值\n  Summary和Histogram十分相似，常用于跟踪事件发生的规模，例如：请求耗时、响应大小。同样提供 count 和 sum 全部值的功能。例如：count=7次，sum=7次的值求值。 它提供一个quantiles的功能，可以按%比划分跟踪的结果。例如：quantile取值0.95，表示取采样值里面的95%数据。  同时对于Histogram的指标，我们还可以通过histogram_quantile()函数计算出其值的分位数。不同在于Histogram通过histogram_quantile函数是在服务器端计算的分位数。 而Sumamry的分位数则是直接在客户端计算完成。因此对于分位数的计算而言，Summary在通过PromQL进行查询时有更好的性能表现，而Histogram则会消耗更多的资源。反之对于客户端而言Histogram消耗的资源更少。在选择这两种方式时用户应该按照自己的实际场景进行选择。  Promql查询 查询时间序列  精确匹配：== != 正则匹配 =~ !~  http_requests_total{environment=~\u0026quot;staging|testing|development\u0026quot;,method!=\u0026quot;GET\u0026quot;}\n范围查询 我们通过``http_requests_total`查询，返回值中只会包含该时间序列中的最新的一个样本值，这样的返回结果我们称为**__瞬时向量__**,而相应的这样的表达式称之为__瞬时向量表达式__.\n如果想去查询一段时间内的样本数据时，我们则需要__区间向量表达式__http_requests_total{}[5m],区间向量单位：\n s - 秒 m - 分钟 h - 小时 d - 天 w - 周 y - 年  时间位移查询 关键字 offset\nhttp_request_total{} offset 5m //5分钟前的瞬时向量 http_request_total{}[1d] offset 1d //昨天一天内的区间向量  聚合操作 Prometheus还提供了下列内置的聚合操作符，这些操作符作用域瞬时向量。可以将瞬时表达式返回的样本数据进行聚合，形成一个新的时间序列。\n\u0026lt;aggr-op\u0026gt;([parameter,] \u0026lt;vector expression\u0026gt;) [without|by (\u0026lt;label list\u0026gt;)]   sum (求和) min (最小值) max (最大值) avg (平均值) stddev (标准差) stdvar (标准方差) count (计数) count_values (对value进行计数) bottomk (后n条时序) topk (前n条时序) quantile (分位数)  例子：\n##计算k8s节点cpu资源 avg (irate(node_cpu_seconds_total[15m]) )by (instance) ##第一步： irate(node_cpu_secondes_total[15m] )获取瞬时向量 ### 第二步：by instance 及把所有instance相同的metric 放在一起取平均值 avg (irate(node_cpu_seconds_total[15m]) )by (instance)  ","id":11,"section":"posts","summary":"[toc] 监控的目的 在《SRE: Google运维解密》一书中指出，监控系统需要能够有效的支持白盒监控和黑盒监控。通过白盒能够了解其内部的实际运行状态","tags":["监控","prometheus"],"title":"Prometheus（一）","uri":"https://zhangshunping.github.io/2020/08/prometheus1/","year":"2020"},{"content":"[TOC]\n Export_execl Purpose Export_execl is my a little tool that Perform export execl tasks and send it as attachment to the designated person’s mailbox according to the needs。Only suport export execl from mysql.\nHow to use it Develop Language python2 , Golang \u0026gt;= 1.14\nstep1 . Compile goEmail by go mod cd sendEmailByGo go mod init sendEmailByGo go build -o goEmail -i main.go cp goEmail /usr/local/bin/ // 拷贝eamil stmp服务配置文件 mkdir -p /root/.email/ cp goEmailExample.json /root/.email/goEmail.json   goEmial help  [root@taliyun-k8s-master01 Export_execl]# goEmail -h Usage of goEmail: -a string 附件路径名字 -b string 邮件内容 (default \u0026quot;弹性伸缩web健康检查失败,健康码是：\u0026quot;) -c string goEmail.json文件存放路径 (default \u0026quot;/root/.email/\u0026quot;) -h\t帮助说明 -s string 邮件主题 (default \u0026quot;弹性伸缩web健康检测失败!\u0026quot;) [root@taliyun-k8s-master01 Export_execl]#  step2. Configuration cat config.ini\nScreenshots  When you complete the Step1 and Step2 configuration, and then you can execute command python export_execl_from_mysql.py  `   Recived an email that has Execl Attachment.  ","id":12,"section":"posts","summary":"[TOC] Export_execl Purpose Export_execl is my a little tool that Perform export execl tasks and send it as attachment to the designated person’s mailbox according to the needs。Only suport export execl from mysql. How to use it Develop Language python2 , Golang \u0026gt;= 1.14 step1 . Compile goEmail by go mod cd sendEmailByGo go","tags":["Golang","Python"],"title":"开源小工具- Export-execl","uri":"https://zhangshunping.github.io/2020/07/export-execl/","year":"2020"},{"content":"[TOC]\nobjectss  objectss 是为大数据大并发迁移到对象存储上而设计开源小项目。\nPurpose  背景： 公司战略问题，需要从阿里云迁移到华为云；使用的git服务器，有10T的存储容量，希望高并发的执行迁移任务，同时将一些不经常使用的git仓库存储在云对象存储上，等到要用时候去上面拉取。 选择：使用golang高并发的执行迁移任务,对接云存储的结构，进行数据传输。  Model 基于生产者多个消费者模型   生产者\n 通过查询sql中记录的gitpath路径，放入到带缓冲区的管道中    消费者\n 根据参数-s 指定的int值(默认10) 启用的消费者go程执行 obs/oss 迁移任务 如果迁移成功，则修改数据中的flag oss=1证明已经迁移到cloud 对象存储上了，如果迁移命令失败 则oss 仍然为0    Help $ go run main.go -h upload files to cloud oss Usage: objectss [command] Available Commands: help Help about any command obs huawei cloud obs oss aliyun object oss Flags: -c, --ChannelCap int channle cap (-c 10) (default 10) -l, --ObjectStorgeLink string oss/obs link (-o oss://educoder.tmp ) (default \u0026quot;oss://educoder.tmp\u0026quot;) --config string config file (default is $HOME/.objectss.yaml) -s, --consusmerNum int Run the number of comsumer goroutines (-s 100) (default 100) -h, --help help for objectss -n, --sqlLimits int sql limit nums (-n 1000) (default 1000) --sqlcon string connect sql (default is $HOME/.objectss.yaml) (default \u0026quot;root:123456789@tcp(127.0 .0.1:3306)/gitlab\u0026quot;) -d, --sqldays int select data from mysql 15 days ago (-d -15) (default -15) -t, --toggle Help message for toggle Use \u0026quot;objectss [command] --help\u0026quot; for more information about a command.  需补充 oss和obs存储迁移的 命令嵌入\n","id":13,"section":"posts","summary":"[TOC] objectss objectss 是为大数据大并发迁移到对象存储上而设计开源小项目。 Purpose 背景： 公司战略问题，需要从阿里云迁移到华为云；使用的git服务器，有10T的存储容","tags":["Golang"],"title":"开源小工具-objectss","uri":"https://zhangshunping.github.io/2020/07/objectss/","year":"2020"},{"content":"我的Gin 项目结构 ├── Config //处理配置文件代码 │ └── cofig1.go ├── Controller //控制层 │ ├── controllerHanlder.go │ ├── DbHandler.go │ └── StaticHandler │ ├── HelloWorld.go │ ├── Info.go │ └── PageNotFound.go ├── go.mod ├── go.sum ├── main.go ├── Middlerwar\t//中间件 │ └── m1.go ├── README.md ├── README.mdls ├── Router\t// 路由 │ ├── apiRouter.go │ ├── SysRouter.go │ └── init_router.go ├── Service\t// 业务逻辑 │ └── service1.go ├── Utils\t// 工具 ├── Model\t// 数据库  定义Router套路  router目录  ├── Router\t// 路由 │ ├── apiRouter.go │ ├── SysRouter.go │ └── init_router.go    init_router.go为init初始化目录\nimport ( \u0026quot;Edu-DevopsWeb/Controller/StaticHandler\u0026quot; \u0026quot;github.com/gin-gonic/gin\u0026quot; ) func InitDefaultRouter(m ...gin.HandlerFunc){ r:=gin.Default() //1.是否使用https //if config2.ApplicationConfig.IsHttps { //\tr.Use(handler.TlsHandler()) //} //2.是否使用中间件 // middleware.InitMiddleware(r) // the jwt middleware //authMiddleware, err := middleware.AuthInit() //tools.HasError(err, \u0026quot;JWT Init Error\u0026quot;, 500) //3、注册系统路由 InitSysRouter(r, authMiddleware) //InitSysRouter(r,authMiddlerware) //4. 注册业务路由 //\tInitExamplesRouter(r, authMiddleware) // Not Router 提示 r.NoRoute(StaticHandler.NotFound) r.Run(\u0026quot;:80\u0026quot;) }    分别定义 base路由和业务路由. 业务路由里又可以定义版本路由，认证路由，不需要认证的路由，这样的话，在以后的CURD过程中，就可以把需要版本控制，权限控制等路由添加到自定的方法中去。\n// 1.顶一个InitSysRouter 初始化业务路由，基本路由，需要check得路由等等 func InitSysRouter( r *gin.Engine){ g:=r.Group(\u0026quot;\u0026quot;) // init base router InitBaseRouter(g) // init 业务router(example) V1(g) // 无需认证 } // 2. 定义基本路由 func InitBaseRouter(g *gin.RouterGroup) { g.GET(\u0026quot;/\u0026quot;,StaticHandler.HelloWorld) g.GET(\u0026quot;/info\u0026quot;,StaticHandler.InfoHandlerFunc) } //3. 定义业务版本管理, func V1(g *gin.RouterGroup,authMiddleware ...gin.HandlerFunc){ g=g.Group(\u0026quot;/v1\u0026quot;) // 注册业务 { InitDbrouter(g,) } } //4.定义业务路由方法 func InitDbrouter(g *gin.RouterGroup, handlerFunc ...gin.HandlerFunc) { g.GET(\u0026quot;/db/\u0026quot;,Controller.DbGetHanderFunc) }    ","id":14,"section":"posts","summary":"我的Gin 项目结构 ├── Config //处理配置文件代码 │ └── cofig1.go ├── Controller //控制层 │ ├── controllerHanlder.go │ ├── DbHandler.go │ └── StaticHandler │ ├── HelloWorld.go │ ├── Info.go │ └── PageNotFound.go ├──","tags":["golang","Gin"],"title":"Gin_Router套路","uri":"https://zhangshunping.github.io/2020/07/gin_router/","year":"2020"},{"content":"[TOC]\n 背景 日常工作发现用kubectl 管理k8s集群虽然方便，但是针对某一些资源的CURD不是很好。 Kubect-addons 实对kubectl 命令的一个补充。使用到的包有 cobra ,go-client , color 。\n使用方法： [root@taliyun-k8s-master01 kubectl-addons]# ./kubectl-addons get nodeanno --help get node Annotation Usage: kubectl-addons get nodeanno [flags] Examples: 1.kubect-addons get nodeanno -a \u0026quot;CA\u0026quot; --\u0026gt; to get ClusterAutoSacler node that not to clam down 2. kubectl-addons get nodeanno -a \u0026quot;All\u0026quot; --\u0026gt; to get all Node Annotation 3. kubectl-addons get nodeanno -a '{\u0026quot;flannel.alpha.coreos.com/backend-type\u0026quot;:\u0026quot;vxlan\u0026quot;}' --\u0026gt; to list given Annotation Node 4. kubectl-addons get nodeanno -a '{\u0026quot;cluster-autoscaler.kubernetes.io/scale-down-disabled\u0026quot;:\u0026quot;true\u0026quot;}' -k C:/Users/39295/kube/config Flags: -a, --annotation string get nodeanno -a '{\u0026quot;flannel.alpha.coreos.com/backend-type\u0026quot;:\u0026quot;vxlan\u0026quot;}' ,to list gien node -h, --help help for nodeanno Global Flags: --config string config file (default is $HOME/.kubectl-addons.yaml) -k, --kubeconfig string -k C:/Users/39295/kube/config (default \u0026quot;/root/.kube/config\u0026quot;) -n, --namespace string -n default (default \u0026quot;default\u0026quot;) -l, --nodeslector string -l type=others --viper use Viper for configuration (default true)  截图  1.get cluster-auto-scaler node that not be scaled down .   2、get all node\u0026rsquo;s annotation in k8s cluster   get some specify annotation by user  ","id":15,"section":"posts","summary":"[TOC] 背景 日常工作发现用kubectl 管理k8s集群虽然方便，但是针对某一些资源的CURD不是很好。 Kubect-addons 实对kubectl 命令的一个补充。使用到的","tags":["Golang"],"title":"开源小工具- Kubectl Addons","uri":"https://zhangshunping.github.io/2020/07/kubectl-addons/","year":"2020"},{"content":"[TOC]\njson渲染 func main() { r := gin.Default() // 1、map渲染，gin.H{} r.GET(\u0026quot;/map_json\u0026quot;, func(c *gin.Context) { c.JSON(200, gin.H{ \u0026quot;message\u0026quot;: \u0026quot;map_json\u0026quot;, }) }) //2、 结构体渲染， r.GET(\u0026quot;/other_json\u0026quot;, func(c *gin.Context) { c.JSON(http.StatusOK,Other_json{Messag: \u0026quot;otherJson\u0026quot;}) }) r.Run() // listen and serve on 0.0.0.0:8080 (for windows \u0026quot;localhost:8080\u0026quot;) } type Other_json struct{ // `json:\u0026quot;name\u0026quot;` 表示用json包来处理的时候，字段名字为name Messag string `json:\u0026quot;name\u0026quot;` }  获取参数 Query 参数  C.GetQuery() and c.DefaultQuery()  r.GET(\u0026quot;/query\u0026quot;, func(c *gin.Context) { // 1、 c.DefaultQuery() value:=c.DefaultQuery(\u0026quot;hello\u0026quot;,\u0026quot;not_hello\u0026quot;) // 2、 c.GetQuery() value,ok:=c.GetQuery(\u0026quot;hello\u0026quot;) if !ok{ value=\u0026quot;not_hello\u0026quot; } c.JSON(http.StatusOK,gin.H{ \u0026quot;Msg\u0026quot;:value, }) })  PostForm 参数  c.DefaultPostForm() and c.GetPostForm()  r.Post(\u0026quot;/query\u0026quot;, func(c *gin.Context) { // 1、 c.DefaultQuery() value:=c.DefaultPostForm(\u0026quot;hello\u0026quot;,\u0026quot;not_hello\u0026quot;) // 2、 c.GetQuery() value,ok:=c.GetPostForm(\u0026quot;hello\u0026quot;) if !ok{ value=\u0026quot;not_hello\u0026quot; } c.JSON(http.StatusOK,gin.H{ \u0026quot;Msg\u0026quot;:value, }) })  Uri 参数 r.GET(\u0026quot;/api/v1/:year/:month\u0026quot; , func(c *gin.Context) { year:=c.Param(\u0026quot;year\u0026quot;) month:=c.Param(\u0026quot;month\u0026quot;) c.JSON(http.StatusOK,gin.H{ \u0026quot;year\u0026quot;:year, \u0026quot;month\u0026quot;:month, }) }) // 浏览器输入 ：http://127.0.0.1:8080/api/v1/2020/07 // 显示结果为 {\u0026quot;month\u0026quot;:\u0026quot;07\u0026quot;,\u0026quot;year\u0026quot;:\u0026quot;2020\u0026quot;}  参数绑定 基于请求的Content-Type识别请求数据类型并利用反射机制自动提取请求中QueryString、form表单、JSON、XML等参数到结构体中。\ntype Login struct { User string `form:\u0026quot;user\u0026quot; json:\u0026quot;user\u0026quot; xml:\u0026quot;user\u0026quot; ` Password string `form:\u0026quot;password\u0026quot; json:\u0026quot;password\u0026quot; xml:\u0026quot;password\u0026quot; ` } func main() { router := gin.Default() // Get query router.GET(\u0026quot;/login\u0026quot;, LoginHandler) // Get json router.GET(\u0026quot;/loginJ\u0026quot;, LoginHandlerJ) // Post query Form router.POST(\u0026quot;/loginF\u0026quot;, LoginHandlerF) router.Run(\u0026quot;:8080\u0026quot;) // listen and serve on 0.0.0.0:8080 (for windows \u0026quot;localhost:8080\u0026quot;) } // query bind //Get--\u0026gt; 127.0.0.1:8080/login?user2=apple\u0026amp;password3=apple1 func LoginHandler(c *gin.Context) { var login Login //if err:=c.ShouldBind(\u0026amp;login);err!=nil{ if err := c.ShouldBindQuery(\u0026amp;login); err != nil { c.JSON(http.StatusBadRequest, err.Error()) return } c.JSON(http.StatusOK, login) } // json bind // 传递 json{ \u0026quot;user\u0026quot;:\u0026quot;123\u0026quot;, \u0026quot;password\u0026quot;:\u0026quot;aple\u0026quot; } func LoginHandlerJ(c *gin.Context) { var login Login if err := c.ShouldBindJSON(\u0026amp;login); err != nil { c.JSON(http.StatusBadRequest, err.Error()) return } c.JSON(http.StatusOK, login) } //Form bind // Post --\u0026gt; 127.0.0.1:8080/loginF?user=apple\u0026amp;password=appple1 func LoginHandlerF(c *gin.Context) { var login Login if err := c.ShouldBindQuery(\u0026amp;login); err != nil { c.JSON(http.StatusBadRequest, err.Error()) return } c.JSON(http.StatusOK, login) }  文件上传  前端Form表单  \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026quot;zh-CN\u0026quot;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;上传文件示例\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;form action=\u0026quot;/upload\u0026quot; method=\u0026quot;post\u0026quot; enctype=\u0026quot;multipart/form-data\u0026quot;\u0026gt; \u0026lt;input type=\u0026quot;file\u0026quot; name=\u0026quot;f1\u0026quot;\u0026gt; \u0026lt;input type=\u0026quot;submit\u0026quot; value=\u0026quot;上传\u0026quot;\u0026gt; \u0026lt;/form\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;   单文件上传  func main() { // 处理multipart forms提交文件时默认的内存限制是32 MiB // 可以通过下面的方式修改 // router.MaxMultipartMemory = 8 \u0026lt;\u0026lt; 20 // 8 MiB\trouter := gin.Default() router.LoadHTMLFiles(\u0026quot;html/upload.html\u0026quot;) router.GET(\u0026quot;/index\u0026quot;, func(c *gin.Context) { c.HTML(http.StatusOK,\u0026quot;upload.html\u0026quot;,nil) }) router.POST(\u0026quot;/upload\u0026quot;,UploadHanderFunc) router.Run(\u0026quot;:8080\u0026quot;) // listen and serve on 0.0.0.0:8080 (for windows \u0026quot;localhost:8080\u0026quot;) } // file,err:=c.FORMFILE(f) //c.SaveUpLoadFile(file,path) func UploadHanderFunc(c *gin.Context){ if file,err:=c.FormFile(\u0026quot;f1\u0026quot;);err !=nil{ c.JSON(http.StatusInternalServerError,err.Error()) return }else{ filePath:=path.Join(\u0026quot;./upload/\u0026quot;,file.Filename) c.SaveUploadedFile(file,filePath) c.JSON(http.StatusOK,gin.H{ \u0026quot;status\u0026quot;:\u0026quot;ok\u0026quot;, }) } }   多个文件  func main() { router := gin.Default() // 处理multipart forms提交文件时默认的内存限制是32 MiB // 可以通过下面的方式修改 // router.MaxMultipartMemory = 8 \u0026lt;\u0026lt; 20 // 8 MiB router.POST(\u0026quot;/upload\u0026quot;, func(c *gin.Context) { // Multipart form form, _ := c.MultipartForm() files := form.File[\u0026quot;file\u0026quot;] for index, file := range files { log.Println(file.Filename) dst := fmt.Sprintf(\u0026quot;C:/tmp/%s_%d\u0026quot;, file.Filename, index) // 上传文件到指定的目录 c.SaveUploadedFile(file, dst) } c.JSON(http.StatusOK, gin.H{ \u0026quot;message\u0026quot;: fmt.Sprintf(\u0026quot;%d files uploaded!\u0026quot;, len(files)), }) }) router.Run() }  路由 重定向  http重定向  router := gin.Default() //1、 HTTP重定向，通过C.Redirect router.GET(\u0026quot;/version1\u0026quot;, func(c *gin.Context) { //跳转到baidu //c.Redirect(http.StatusMovedPermanently,\u0026quot;https://www.baidu.com\u0026quot;) //跳转到v2 c.Redirect(http.StatusMovedPermanently,\u0026quot;v2\u0026quot;) }) router.GET(\u0026quot;/v2\u0026quot;, func(c *gin.Context) { c.JSON(http.StatusPermanentRedirect,gin.H{ \u0026quot;status\u0026quot;:\u0026quot;ok\u0026quot;, \u0026quot;version\u0026quot;:\u0026quot;v2\u0026quot;, }) })   路由重定向  //2、 路由重定向 (跳转到c2，但是url还是127.0.0.1/v1 router.GET(\u0026quot;/v1\u0026quot;, func(c *gin.Context) { c.Request.URL.Path=\u0026quot;/v2\u0026quot; router.HandleContext(c) }) router.GET(\u0026quot;/v2\u0026quot;, func(c *gin.Context) { c.JSON(http.StatusPermanentRedirect,gin.H{ \u0026quot;status\u0026quot;:\u0026quot;ok\u0026quot;, \u0026quot;version\u0026quot;:\u0026quot;v2\u0026quot;, }) })  路由组 func main() { router := gin.Default() // 定义路由组 以/www为开头的 IndexGroup := router.Group(\u0026quot;/www\u0026quot;) { // 通过any() switch定义restful , // 访问的地址为uri为 /www/list/ IndexGroup.Any(\u0026quot;/list\u0026quot;, func(c *gin.Context) { switch c.Request.Method { // 查 case http.MethodGet: c.JSON(http.StatusOK, gin.H{ \u0026quot;status\u0026quot;: \u0026quot;ok\u0026quot;, \u0026quot;Method\u0026quot;: http.MethodGet, }) // 增 case http.MethodPost: c.JSON(http.StatusOK, gin.H{ \u0026quot;status\u0026quot;: \u0026quot;ok\u0026quot;, \u0026quot;Method\u0026quot;: http.MethodPost, }) //改 case http.MethodPut: c.JSON(http.StatusOK, gin.H{ \u0026quot;status\u0026quot;: \u0026quot;ok\u0026quot;, \u0026quot;Method\u0026quot;: http.MethodPut, }) // 删 case http.MethodDelete: c.JSON(http.StatusOK, gin.H{ \u0026quot;status\u0026quot;: \u0026quot;ok\u0026quot;, \u0026quot;Method\u0026quot;: http.MethodDelete, }) } }) } // No route，定义无路由规则 router.NoRoute(func(c *gin.Context) { c.JSON(http.StatusNotFound, gin.H{ \u0026quot;Err\u0026quot;: \u0026quot;Page NOt Found!\u0026quot;, }) }) router.Run(\u0026quot;:80\u0026quot;) // listen and serve on 0.0.0.0:8080 (for windows \u0026quot;localhost:8080\u0026quot;) }   路由组嵌套  router := gin.Default() // 定义路由组 IndexGroup := router.Group(\u0026quot;/www\u0026quot;) IndexGroup.Any(\u0026quot;/list\u0026quot;, func(c *gin.Context) { switch c.Request.Method { // 查 case http.MethodGet: c.JSON(http.StatusOK, gin.H{ \u0026quot;status\u0026quot;: \u0026quot;ok\u0026quot;, \u0026quot;Method\u0026quot;: http.MethodGet, }) } }) // 嵌套 xx:=IndexGroup.Group(\u0026quot;ex\u0026quot;) // 访问地址为: http://127.0.0.1/www/ex/ xx.GET(\u0026quot;/\u0026quot;,xxHandelr)  中间件 允许框架的使用者在使用框架的时候，加入自己的一些定义的hook函数。（使用处理一些公共的资源），比如登陆登陆验证，权限认证，数据分页，记录日志，耗时统计等。\n自定义中间件 Gin中的中间件必须是一个gin.HandlerFunc类型\n// HandlerFunc defines the handler used by gin middleware as return value. type HandlerFunc func(*Context)  //一般用闭包定义中间件 func authUserM1(doChe bool) gin.HandlerFunc{ if doChe{ //1. 查询数据库 逻辑 //2. 校验成功逻辑 return func(c *gin.Context) {fmt.Println(\u0026quot;auth ok\u0026quot;)\t;c.Next()} } else { return func(c *gin.Context) {fmt.Println(\u0026quot;auth failed\u0026quot;)\t;c.Abort()} } }  中间件常见使用  处理流程  b--\u0026gt;m1Func ---\u0026gt;m2Func----\u0026gt;handerFunc b\u0026lt;--m1Func\u0026lt;----m2Func\u0026lt;----handerFunc //c.Next() 表示执行后面的Func //c.Abort() 表示终止后面的Func,直接按原路返回 router.Get(\u0026quot;/\u0026quot;,m1,m2,Handerfunc) func m1(c *gin.context){ fmt.Println(\u0026quot;m1 start\u0026quot;) c.Next() fmt.Println(\u0026quot;m1 end\u0026quot;) } func m2(c *gin.context){ fmt.Println(\u0026quot;m2 start\u0026quot;) c.Next() fmt.Println(\u0026quot;m2\u0026quot;) } // 输出结果为 \u0026quot;m1 start , m2 start , m2 end , m1 end\u0026quot;   伪代码模仿认证中间件  func main() { router:=gin.Default() //router.User() router.Use(authUserM1(true)) // 全局注册 //路由组注册 router.Use() Grouproute:=router.Group(\u0026quot;/\u0026quot;) Grouproute.Use(authUserM1(true)) //路由注册 router.GET(\u0026quot;/\u0026quot;,authUserM1(true), func(c *gin.Context) {c.JSON(http.StatusOK,gin.H{ \u0026quot;status\u0026quot;:\u0026quot;ok\u0026quot;, }) }) } func authUserM1(doChe bool) gin.HandlerFunc{ if doChe{ //1. 查询数据库 逻辑 //2. 校验成功逻辑 return func(c *gin.Context) {fmt.Println(\u0026quot;auth ok\u0026quot;)\t;c.Next()} } else { return func(c *gin.Context) {fmt.Println(\u0026quot;auth failed\u0026quot;)\t;c.Abort()} } }    注意事项\ngin默认中间件\ngin.Default()默认使用了Logger和Recovery中间件，其中：\n Logger中间件将日志写入gin.DefaultWriter，即使配置了GIN_MODE=release。 Recovery中间件会recover任何panic。如果有panic的话，会写入500响应码。  如果不想使用上面两个默认的中间件，可以使用gin.New()新建一个没有任何默认中间件的路由。\ngin中间件中使用goroutine\n当在中间件或handler中启动新的goroutine时，不能使用原始的上下文（c *gin.Context），必须使用其只读副本（c.Copy()）。\n  ","id":16,"section":"posts","summary":"[TOC] json渲染 func main() { r := gin.Default() // 1、map渲染，gin.H{} r.GET(\u0026quot;/map_json\u0026quot;, func(c *gin.Context) { c.JSON(200, gin.H{ \u0026quot;message\u0026quot;: \u0026quot;map_json\u0026quot;, }) }) //2、 结构体渲染， r.GET(\u0026quot;/other_json\u0026quot;, func(c *gin.Context) { c.JSON(http.StatusOK,Other_json{Messag: \u0026quot;otherJson\u0026quot;}) }) r.Run() // listen and serve on 0.0.0.0:8080 (for windows \u0026quot;localhost:8080\u0026quot;) } type Other_json struct{","tags":["golang","Gin"],"title":"Gin框架常见用法","uri":"https://zhangshunping.github.io/2020/07/gin_%E5%B8%B8%E8%A7%81%E7%94%A8%E6%B3%95/","year":"2020"},{"content":"[TOC]\nContext诞生背景 由于在Golang severs中，每个request都是在单个goroutine中完成，并且在单个goroutine（不妨称之为A）中也会有请求其他服务（启动另一个goroutine（称之为B）去完成）的场景，这就会涉及多个Goroutine之间的调用。如果某一时刻请求其他服务被取消或者超时，则作为深陷其中的当前goroutine B需要立即退出，然后系统才可回收B所占用的资源。 即一个request中通常包含多个goroutine，这些goroutine之间通常会有交互。\n那么，如何有效管理这些goroutine成为一个问题（主要是退出通知和元数据传递问题），Google的解决方法是Context机制，相互调用的goroutine之间通过传递context变量保持关联，这样在不用暴露各goroutine内部实现细节的前提下，有效地控制各goroutine的运行。\n如此一来，通过传递Context就可以追踪goroutine调用树，并在这些调用树之间传递通知和元数据。 虽然goroutine之间是平行的，没有继承关系，但是Context设计成是包含父子关系的，这样可以更好的描述goroutine调用之间的树型关系。\ncontext使用方式   Context是一个接口\ntype Context interface { Deadline() (deadline time.Time, ok bool) // 超时 Done() \u0026lt;-chan struct{} // 退出channel Err() error // err报错 Value(key interface{}) interface{} // context上下文value传递 }    context.go包自带的常用三种初始化方式\nctx,cancel:=context.WithCancel(context.Background()) ctx,cancel:=context.WithDeadline(context.Background(),time.Now().Add(time.Second*10)) ctx,cancle:=context.WithTimeout(context.Background(),time.Second*10) //返回是一个ctx Context，和一个cancel 函数体 //ctx有三种方式    context.withValue（）使用 trace_code的场景\nvar ( wg sync.WaitGroup ) // 按官方标准建立自己定义type type ContextTc struct { Traceid string } func main(){ traceid :=ContextTc{Traceid: \u0026quot;Trace_code\u0026quot;} // ctx 上下文初始化 ctx,cancle:=context.WithTimeout(context.Background(),time.Second*10) ctx=context.WithValue(ctx,traceid.Traceid,\u0026quot;1\u0026quot;) defer cancle() wg.Add(1) go worker(ctx,traceid.Traceid) wg.Wait() } func worker(ctx context.Context,traceid string){ select { //超市或者cacncle()时，等待ctx.Done() case \u0026lt;-ctx.Done(): fmt.Println(\u0026quot;context stop and exit\u0026quot;) return default: log.Printf(\u0026quot;Trace_code %s: working\u0026quot;,ctx.Value(traceid)) } defer wg.Done() }    ","id":17,"section":"posts","summary":"[TOC] Context诞生背景 由于在Golang severs中，每个request都是在单个goroutine中完成，并且在单个goroutine","tags":null,"title":"Golang-Context","uri":"https://zhangshunping.github.io/2020/06/golang-context/","year":"2020"},{"content":"[toc]\n定义 所谓闭包指的是内部函数引用外部函数变量或者自由变量返回一个函数，我们称作为闭包\nfunc outer(x int )func(int) int{ // golang一个函数体内只能用用一个函数体,因此使用匿名函数 return func(y int) int{ return x+y } } func main() { f:=outer(1) fmt.Println(f(100)) }  闭包场景一： for-range使用闭包 第一个示列： func main() { a:=[]string{\u0026quot;a\u0026quot;,\u0026quot;b\u0026quot;,\u0026quot;C\u0026quot;} for _,i:=range(a){ go func(){ fmt.Println(i) }() } time.Sleep(1*time.Second) }  输出结果居然不是：a,b,c 而是c，c，c 。\n第二个示列 func main() { a:=[]string{\u0026quot;a\u0026quot;,\u0026quot;b\u0026quot;,\u0026quot;C\u0026quot;} for _,i:=range(a){ go func(i string){ fmt.Println(i) }(i) } time.Sleep(1*time.Second) }  输出结果跟预期的一样。\n说明： 第一个示列，主进程跑的过快，go程里打印的i是值引用，因此打印的结果为i的最后一个值。而第二个示列中，直接传i值，因此打出的结果符合预期，当然我们可以针对示列一做如下修改,输出结果依然符合预期\nfunc main() { a:=[]string{\u0026quot;a\u0026quot;,\u0026quot;b\u0026quot;,\u0026quot;C\u0026quot;} for _,i:=range(a){ go func(){ fmt.Println(i) }() time.Sleep(1*time.Second) } time.Sleep(4*time.Second) }  闭包场景二：函数列表闭包 第三个示列 func funcslic() []func(){ s:=[]func(){} for i:=0; i\u0026lt;3;i++{ s=append(s, func() { fmt.Println(\u0026amp;i,i) }) } return s } func main(){ Fs:=funcslic() for _,k:=range(Fs){ k() } }  输出结果为：0x11056058 3 0x11056058 3 0x11056058 3\n第四个示列 func funcslic() []func(){ s:=[]func(){} for i:=0; i\u0026lt;3;i++{ x:=i s=append(s, func() { fmt.Println(\u0026amp;x,x) }) } return s } func main(){ Fs:=funcslic() for _,k:=range(Fs){ k() } }  输出结果为： 0x11056058 0 0x1105605c 1 0x11056070 2\n说明： 每次 append 操作仅将匿名函数放入到列表中，但并未执行，并且引用的变量都是 i，随着 i 的改变匿名函数中的 i 也在改变，所以当执行这些函数时，他们读取的都是环境变量 i 最后一次的值。解决的方法就是每次复制变量 i 然后传到匿名函数中(及为局部变量），让闭包的环境变量不相同。因此我们就可以用闭包来修改环境变量如下：\nvar a int =10 func main(){ go func(){ a=a+2 }() fmt.Println(a) } //怎么样，是不是觉得这段代码，经常使用。  闭包场景三：延迟调用defer 第五个示列 package main import \u0026quot;fmt\u0026quot; func main(){ var x,y int=1,2 defer func(i int){ fmt.Println(\u0026quot;defer\u0026quot;,i,y) }(x) x=100 y=100 fmt.Println(\u0026quot;main\u0026quot;,x,y) } //输出结果 $ go run main.go main 100 100 defer 1 100  闭包场景四：return, defer 第五个示列 不带命名的返回值\npackage main import \u0026quot;fmt\u0026quot; var i int func test() (int) { defer func(){ i++ fmt.Println(\u0026quot;defer1\u0026quot;,i) }() defer func(){ i++ fmt.Println(\u0026quot;defer2\u0026quot;,i) }() return i } func main() { fmt.Println(\u0026quot;test return\u0026quot;,test()) }  输出结果为：defer2 1 defer1 2 test return 0\n第六个示列 带命名的返回值\npackage main import \u0026quot;fmt\u0026quot; func test() (i int) { defer func(){ i++ fmt.Println(\u0026quot;defer1\u0026quot;,i) }() defer func(){ i++ fmt.Println(\u0026quot;defer2\u0026quot;,i) }() return i } func main() { fmt.Println(\u0026quot;test return\u0026quot;,test()) }  输出结果为：\n$ go run main.go defer2 1 defer1 2 test return 2\ndefer，return 说明： return 是一个并非原子操作，分别为赋值和返回值两步操作。return i 可以看作 j:=i 返回j\n不带命名的返回值： return 最先执行，此时i=0,j:=i，返回的值为j；defer2后入栈，先出i=i+1，defer1先进，后出i=1+1\n带命名的返回值：\nreturn 最先执行，此时i=0,j:=i，返回的是j；defer2后入栈，先出i=i+1，defer1先进，后出i=1+1 ，逻辑跟上面一样，但是命名的返回值是i，而不是j，所以结果是2\n总结： 闭包紧盯着外部变量; defer语句，优先入栈。\n","id":18,"section":"posts","summary":"[toc] 定义 所谓闭包指的是内部函数引用外部函数变量或者自由变量返回一个函数，我们称作为闭包 func outer(x int )func(int) int{ // golang一个函数体内只能用用一个函数体,","tags":["golang"],"title":"Golang-闭包场景总结","uri":"https://zhangshunping.github.io/2020/06/golang-%E9%97%AD%E5%8C%85%E5%9C%BA%E6%99%AF%E6%80%BB%E7%BB%93/","year":"2020"},{"content":"[TOC]\n需求背景 ​\t公司的2B业务落地后，应用服务因为一些突发状况出现不在线情况。\n  一些小型客户关注的是SAAS业务，不太关心运维体系，也没有充裕的资源建立一套运维系统。\n  开源组件建立本地版运维体系，较为烦躁。\n  希望能够建立一套健康检测系统，在服务出现问题是，能够到连接到指定的机器上执行HOOK命令；同时根据用户实际情况，建立通知系统。\n由于最近半年在golang方面的东西，因此用golang语言写了一个Healthy健康检测服务，取名为Healthy\n  介绍 Healthy是一个通过golang语言写的服务健康检测服务，再发现连续报错之后会通过ssh协议链接指定的主机上执行Hook指令。同时根据用户自定义的Email开关，实现邮件通知（故障报警，恢复通知，报警冷却）。\n软件架构 - Appservice 服务注册，检测，执行Hook - Sshconnect ssh模块 - Email 报警邮件  Hook操作函数： - 执行cmd命令 - 执行通知事件  安装教程 git clone https://gitee.com/zhangshunping123/Healthy.git go build -o healthy -i main.go chmod +x healthy mv healthy /usr/bin/  使用说明 mkdir -p ~/.health/ cp config.json ~/.health/ $ ./main.exe -h Usage of C:\\Users\\39295\\Desktop\\Healthy\\main.exe: -c string Configfile for service health detection (default \u0026quot;~/.health/config.json\u0026quot;) -e int Number of consecutive health test failures (default 10) -h help -i int Health check interval (default 2)  需要补充  hook interface化 多种 通知方式接入 视图友好  ","id":19,"section":"posts","summary":"[TOC] 需求背景 ​ 公司的2B业务落地后，应用服务因为一些突发状况出现不在线情况。 一些小型客户关注的是SAAS业务，不太关心运维体系，也没有充裕的资","tags":["Golang"],"title":"Healthy","uri":"https://zhangshunping.github.io/2020/06/%E5%B0%8F%E5%B7%A5%E5%85%B7-projecthealthy/","year":"2020"},{"content":"[TOC]\n channel，Deadlock死锁的本质 **主协程程阻塞，系统一直等待，导致系统死锁**  场景一： 主协程中使用channel // 1.1 无缓冲channel，读 ch1 := make(chan string) \u0026lt;- ch1 fmt.Println(\u0026quot;ok\u0026quot;) // 1.2 无缓冲channel，写 func main(){ ch1 := make(chan string) ch1 \u0026lt;- \u0026quot;ok\u0026quot; fmt.Println(\u0026quot;ok\u0026quot;) } ##说明： 是因为在同一个主协程中，无缓冲的channel 读和写都是阻塞的，导致系统等待超时，导致deadlock // 2.1 有缓冲读写 func main(){ ch1 := make(chan string,1) ch1 \u0026lt;- \u0026quot;ok\u0026quot; ch1 \u0026lt;- \u0026quot;ok2\u0026quot; fmt.Println(\u0026quot;ok\u0026quot;) } ## 说明： ch1为戴缓冲区1的channel ，写一个ok的时候，主协程不阻塞，写第二个ok2的时候，此时ch1可以看作无缓冲channel，及出现主协程阻塞，导致系统等待，从而deadlock ///修复 //2.1 go 程中使用channel取值 func main(){ ch1 := make(chan string) go func(){ fmt.Println(\u0026quot;ok\u0026quot;,\u0026lt;- ch1) }() ch1\u0026lt;-\u0026quot;ok\u0026quot; time.Sleep(time.Second*1) } //2.2 go程使用channel 赋值 func main(){ ch1 := make(chan string) go func(){ ch1\u0026lt;-\u0026quot;ok\u0026quot; }() fmt.Println(\u0026quot;get from go channle\u0026quot;,\u0026lt;-ch1) } //2.3 注意,如果2.2这样写又会死锁，因为\u0026lt;-ch1在阻塞，导致主协程系统等待 func main(){ ch1 := make(chan string) go func(){ ch1\u0026lt;-\u0026quot;ok\u0026quot; }() fmt.Println(\u0026quot;get from go channle\u0026quot;,\u0026lt;-ch1) fmt.Println(\u0026quot;get2 from go channle\u0026quot;,\u0026lt;-ch1) } //2.4 针对2.3的情况,go程close(ch1)，这样主协程及不会出现等待的情况 func main(){ ch1 := make(chan string) go func(){ ch1\u0026lt;-\u0026quot;ok\u0026quot; close(ch1) }() fmt.Println(\u0026quot;get from go channle\u0026quot;,\u0026lt;-ch1) fmt.Println(\u0026quot;get2 from go channle\u0026quot;,\u0026lt;-ch1) }  场景二：channel遍历导致的deadlock //3.1 range遍历 func main(){ chs := make(chan string, 2) chs \u0026lt;- \u0026quot;first\u0026quot; chs \u0026lt;- \u0026quot;second\u0026quot; for ch := range chs { fmt.Println(ch) } } // 3.2 range遍历修复deadlock func main(){ chs := make(chan string, 2) chs \u0026lt;- \u0026quot;first\u0026quot; chs \u0026lt;- \u0026quot;second\u0026quot; for i:=0;i\u0026lt;=len(chs);i++{ fmt.Println(\u0026lt;-chs) } } //3.3 修复range遍历deadlocak func main(){ chs := make(chan string, 2) chs \u0026lt;- \u0026quot;first\u0026quot; chs \u0026lt;- \u0026quot;second\u0026quot; close(chs) for ch := range chs { fmt.Println(ch) } }  场景三：生产者消费者模型Deadlock //4.1 deadlock func main(){ ch1:=make(chan interface{}) quit:=make(chan bool) // producer go func(){ for i:=0;i\u0026lt;=10;i++{ ch1\u0026lt;-i } }() // comsumer go func(){ for{ fmt.Println(\u0026quot;生产者生产为\u0026quot;,\u0026lt;-ch1) } quit\u0026lt;-true }() \u0026lt;-quit } //说明：是因为quit\u0026lt;-true一直被\u0026lt;-ch1阻塞了，导致了主协程的quit一直阻塞 //4.2改进 func main(){ ch1:=make(chan int) quit:=make(chan bool) // producer go func(){ for i:=0;i\u0026lt;=10;i++{ ch1\u0026lt;-i } }() // comsumer go func(){ for{ select { case num:=\u0026lt;-ch1: fmt.Println(\u0026quot;从管道获取的value是\u0026quot;,num) case \u0026lt;-time.After(3*time.Second): goto loop } } loop: fmt.Println(\u0026quot;loop\u0026quot; ) quit\u0026lt;-true }() \u0026lt;-quit }  ","id":20,"section":"posts","summary":"[TOC] channel，Deadlock死锁的本质 **主协程程阻塞，系统一直等待，导致系统死锁** 场景一： 主协程中使用channel // 1.1 无缓冲ch","tags":["golang-channel","golang"],"title":"Golang-Channel死锁场景总结","uri":"https://zhangshunping.github.io/2020/06/channel-%E6%AD%BB%E9%94%81%E5%9C%BA%E6%99%AF%E6%80%BB%E7%BB%93/","year":"2020"},{"content":"[ToC]\n目的 在实际的生产中能够熟练的使用awk ，sed ，grep等文本操作工具，对故障的快速定位，日志分析等有很大帮助，于是根据自己的实际的生产经验，总结一下awk的用法。\nawk 基本结构 awk 'BEGIN{ print \u0026quot;start\u0026quot; } pattern{ commands } END{ print \u0026quot;end\u0026quot; }' file  一个awk脚本通常由：BEGIN语句块、能够使用模式匹配的通用语句块、END语句块3部分组成，这三个部分是可选的。任意一个部分都可以不出现在脚本中，脚本通常是被单引号或双引号中，例如：\nawk 'BEGIN{ i=0 } { i++ } END{ print i }' filename ##上面的方法跟cat filename|wc -l 的效果一样  awk 常见内置变量 $n 当前记录的第n个字段，比如n为1表示第一个字段，n为2表示第二个字段。 $0 这个变量包含执行过程中当前行的文本内容。 [A] FILENAME 当前输入文件的名。 [A] FS 字段分隔符（默认是任何空格）。 [A] NF 表示字段数，在执行过程中对应于当前的字段数。 [A] NR 表示记录数，在执行过程中对应于当前的行号。 [A] OFMT 数字的输出格式（默认值是%.6g）。 [A] OFS 输出字段分隔符（默认值是一个空格）。 [A] ORS 输出记录分隔符（默认值是一个换行符）。 [A] RS 记录分隔符（默认是一个换行符）。  NR和FNR   NR：表示awk开始执行程序后所读取的数据行数。\nFNR：awk当前读取的记录数，其变量值小于等于NR（比如当读取第二个文件时，FNR是从0开始重新计数，而NR不会）。\n    NR==FNR：用于在读取两个或两个以上的文件时，判断是不是在读取第一个文件\n[root@k8s-master01 tmp]# cat a 张三|000001 李四|000002 [root@k8s-master01 tmp]# cat b 000001|10 000001|20 000002|30 000002|15 [root@k8s-master01 tmp]# awk -F '|' 'NR==FNR{a[$2]=$0;next}{print a[$1]\u0026quot;|\u0026quot;$2,$0}' a b 张三|000001|10 000001|10 张三|000001|20 000001|20 李四|000002|30 000002|30 李四|000002|15 000002|15    打印指定行数\n[root@k8s-master01 tmp]# awk '{if(NR==3)print NR\u0026quot;:\u0026quot;$0}' a.txt 3:hello mysql redis ceph [root@k8s-master01 tmp]# awk 'NR==3 {print NR\u0026quot;:\u0026quot;$0}' a.txt 3:hello mysql redis ceph    NF   NF和$NF\nNF表示字段数，在执行过程中对应于当前的字段数。$NF表示最后一列，$(NF_1)表示倒数第二列\n[root@k8s-master01 tmp]# cat a|awk -F \\| '{print $0}' 张三|000001 李四|000002 [root@k8s-master01 tmp]# cat a|awk -F \\| '{print $NF}' 000001 000002 [root@k8s-master01 tmp]# cat a|awk -F \\| '{print $(NF-1)}' 张三 李四 [root@k8s-master01 tmp]# cat a 张三|000001 李四|000002 李四|000002|000004 [root@k8s-master01 tmp]# awk -F \\| '{printf \u0026quot;文件夹%s,第%s行的列个数是%s \\n\u0026quot;, FILENAME,NR,NF}' a 文件夹a,第1行的列个数是2 文件夹a,第2行的列个数是2 文件夹a,第3行的列个数是3    -v 传递外部变量 [root@k8s-master01 tmp]# awk -v asd=\u0026quot;asdasd\u0026quot; -F \\| '{printf \u0026quot;文件夹%s,第%s行的列个数是%s, %s\\n\u0026quot;, FILENAME,NR,NF,asd }' a 文件夹a,第1行的列个数是2, asdasd 文件夹a,第2行的列个数是2, asdasd 文件夹a,第3行的列个数是3, asdasd  awk运算与判断 算术运算符    运算符 描述     + - 加，减   * / \u0026amp; 乘，除与求余   + - ! 一元加，减和逻辑非   ^ *** 求幂   ++ \u0026ndash; 增加或减少，作为前缀或后缀    [root@k8s-master01 tmp]# awk 'BEGIN{a=0}{print a++,++a}' 0 2 2 4 4 6  逻辑运算符    运算符 描述     || 逻辑或   \u0026amp;\u0026amp; 逻辑与    [root@k8s-master01 tmp]# awk 'BEGIN{a=10;b=1;if (a\u0026gt;5 \u0026amp;\u0026amp; a\u0026lt;10) ;print a}' 10 [root@k8s-master01 tmp]# awk 'BEGIN{a=10;b=1;(a\u0026gt;5 \u0026amp;\u0026amp; a\u0026lt;10) ;print a}' 10  正则运算符    运算符 描述     ~ ~! 匹配正则表达式和不匹配正则表达式    [root@k8s-master01 tmp]# awk 'BEGIN{a=\u0026quot;abferbsdasd\u0026quot; ; if (a ~ /^ab/){print a}}' abferbsdasd [root@k8s-master01 tmp]# awk 'BEGIN{a=\u0026quot;sdferbsdasd\u0026quot; ; if (a ~ /^ab/){print a}}'  其它运算符    运算符 描述     $ 字段引用   空格 字符串连接符   ?: C条件表达式   in 数组中是否存在某键值    awk高级输入输出 next 用法 awk中next语句使用：在循环逐行匹配，如果遇到next，就会跳过当前行，直接忽略下面语句。而进行下一行匹配。next语句一般用于多行合并,next是跳出，类似于continue，不执行后面表达式。\n  打印偶数行\n## 第一种方法 [root@k8s-master01 tmp]# awk '{if (NR%2==0){printf \u0026quot;这是第%s行，打印最后一列是%s \\n\u0026quot;, NR,$NF}}'\\n a.txt 这是第2行，打印最后一列是cloudNative 这是第4行，打印最后一列是kafak 这是第6行，打印最后一列是c 这是第8行，打印最后一列是Django ## 第二种方法用next [root@k8s-master01 tmp]# awk '{if (NR%2==1){next}else{printf \u0026quot;这是第%s行，打印最后一列是%s \\n\u0026quot;, NR,$NF}}'\\n a.txt 这是第2行，打印最后一列是cloudNative 这是第4行，打印最后一列是kafak 这是第6行，打印最后一列是c 这是第8行，打印最后一列是Django    数组应用 数组是awk的灵魂，处理文本中最不能少的就是它的数组处理。因为数组索引（下标）可以是数字和字符串在awk中数组叫做关联数组(associative arrays)。awk 中的数组不必提前声明，也不必声明大小。数组元素用0或空字符串来初始化，这根据上下文而定。\n## 统计netstat 状态链接数 [root@k8s-master01 tmp]# netstat -n|awk '/^tcp/ {++S[$NF]} END{for( a in S) print a , S[a]}' ESTABLISHED 250 TIME_WAIT 896 ### /^tcp/ 匹配TCp开头的 ## ++S[$NF] 相当于 s[$NF]=S[$NF]+1 及 0+1，计数tcp状态 ## END 最后输出  时间函数    格式 描述     函数名 说明   mktime( YYYY MM dd HH MM ss[ DST]) 生成时间格式   strftime([format [, timestamp]]) 格式化时间输出，将时间戳转为时间字符串 具体格式，见下表.   systime() 得到时间戳,返回从1970年1月1日开始到当前时间(不计闰年)的整秒数    建指定时间(mktime使用）\nawk 'BEGIN{tstamp=mktime(\u0026quot;2001 01 01 12 12 12\u0026quot;);print strftime(\u0026quot;%c\u0026quot;,tstamp);}' 2001年01月01日 星期一 12时12分12秒 awk 'BEGIN{tstamp1=mktime(\u0026quot;2001 01 01 12 12 12\u0026quot;);tstamp2=mktime(\u0026quot;2001 02 01 0 0 0\u0026quot;);print tstamp2-tstamp1;}' 2634468  求2个时间段中间时间差，介绍了strftime使用方法\nawk 'BEGIN{tstamp1=mktime(\u0026quot;2001 01 01 12 12 12\u0026quot;);tstamp2=systime();print tstamp2-tstamp1;}' 308201392  常见实列 查看tcp链接状况统计 netstat -n|awk '/^tcp/ {++S[$NF]} END{for (i in S) print i,S[i]}'  日志分析(nginx) Nginx内容如下：\n172.16.95.6 - - [27/Jul/2020:00:00:02 +0800] \u0026quot;GET /api/users/system_update.json HTTP/1.0\u0026quot; 200 23 \u0026quot;https://www.educoder.net/classrooms\u0026quot; \u0026quot;Mozilla/5.0 (Linux; U; Android 8.0.0; zh-cn; CMR-W09 Build/HUAWEICMR-W09) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/66.0.3359.126 MQQBrowser/10.4 Mobile Safari/537.36\u0026quot;    统计日志中访问最多的10个IP\n[root@k8s-master01 ~]# awk '{++S[$1]} END{for (i in S) print i,S[i]}' access.log |sort -k 2 -nr|head -10 94.102.50.96 14 172.16.95.6 7 100.122.65.87 6 100.122.64.243 6 100.122.63.246 6 71.6.167.142 5 100.122.64.206 5    统计日志中访问大于4次的IP\n[root@k8s-master01 ~]# awk '{++S[$1]} END{for (i in S) print i,S[i]}' access.log |sort -k 2 -nr|head -10 94.102.50.96 14 172.16.95.6 7 100.122.65.87 6 100.122.64.243 6 100.122.63.246 6 71.6.167.142 5 100.122.64.206 5 100.122.64.156 5 100.122.63.233 5 222.240.101.70 4    统计某个时间区间内访问最多的10个IP\n[root@k8s-master01 ~]# awk '$4 \u0026gt;\u0026quot;[27/Jul/2020:00:00:00\u0026quot; \u0026amp;\u0026amp; $4 \u0026lt;\u0026quot;[27/Jul/2020:00:00:03\u0026quot; {++S[$1]} END{for (i in S) print i, S[i]} ' access.log |sort -k 2 -rn|head -10 172.16.95.6 6 172.16.95.29 2    统计访问的uri 个数排名\n[root@k8s-master01 ~]# awk '{a[$7]++} END{for (i in a) print a[i],i}' access.log |sort -k 1 -nr |head -10 447 / 5 /forums/categories/5 4 /scripts/setup.php 4 /MyAdmin/scripts/setup.php 4 400 3 /robots.txt 3 /pma/scripts/setup.php 2 /phpMyAdmin/scripts/setup.php 2 http://www.baidu.com/cache/global/img/gs.gif 2 http://clientapi.ipip.net/echo.php?info=1234567890    统计每个URL访问内容的总大小（$body_bytes_sent）\n[root@k8s-master01 ~]# awk '{++S[$7];size[$7]+=$10}END{for(i in S) print size[i],i}' access.log |sort -k 1 -nr|head -10 67646 /images/avatars/LaboratorySetting/1tab?t=1581490135 37406 / 2855 /forums/categories/5 2201 /api/attachments/920111 740 /favicon.ico 676 /scripts/setup.php 676 /MyAdmin/scripts/setup.php 612 http://www.baidu.com/ 571 http://www.qq.com/404/search_children.js 507 /pma/scripts/setup.php    统计访问状态码为404的IP及出现次数\n[root@k8s-master01 ~]# grep 400 access.log |awk '{++S[$1]} END{for (i in S) print S[i],i,\u0026quot;Status:400\u0026quot;}' |sort -k 1 -nr 2 120.132.3.65 Status:400 1 59.36.132.222 Status:400 1 213.227.141.152 Status:400 1 185.53.88.54 Status:400 1 110.249.212.46 Status:400    文件内容对比差异处理   对比文件内容，对比文件差异\n## 方法一 awk 'FNR==NR{a[$0];next} !($0 in a)' a b ##方法二 grep -vf a b    ","id":21,"section":"posts","summary":"[ToC] 目的 在实际的生产中能够熟练的使用awk ，sed ，grep等文本操作工具，对故障的快速定位，日志分析等有很大帮助，于是根据自己的实际的生产经","tags":["awk","linux"],"title":"awk工具使用总结","uri":"https://zhangshunping.github.io/2020/05/awk/","year":"2020"},{"content":"四表五链 四表 五链   filter\u0026mdash; 过滤数据包\n  nat \u0026mdash; 用于网络地址转换（IP、端口）\n  raw \u0026mdash; 决定数据包是否被状态跟踪机制处理\n  mangle \u0026mdash;修改数据包的服务类型、TTL、并且可以配置路由实现QOS\n  五链PREROUTING ,INPUT ,OUTPUT ,FORWARD,POSTROUTING\n  命令（增删改查） iptables 查询  -L   iptables -t 表名 -vnL 链名 不指定表名，及为filter表。\n  iptables -vnL --line-number v显示详细说明，n不需要dns解析，\u0026ndash;line-number显示num。\n[root@local-master ~]# iptables -vnL --line-number Chain INPUT (policy ACCEPT 1453 packets, 109K bytes) num pkts bytes target prot opt in out source destination Chain FORWARD (policy ACCEPT 0 packets, 0 bytes) num pkts bytes target prot opt in out source destination Chain OUTPUT (policy ACCEPT 2108 packets, 146K bytes) num pkts bytes target prot opt in out source destination  //字典解释 pkts：表示匹配的报文个数 bytes:表示报文包的大小 traget:表示规则对应的动作 prot：表示协议 in： 从哪个接口(网卡)流入 out：从哪个接口（网卡）流出 source：表示对应的源头地址，可以是ip也可以是网段 destination：表示对应的目标地址，可以是ip也可以是网段    iptables 管理   增 -I 或者-A\niptables -t 表名 -I 链名 -s -d -j  在首行增加\niptables -t 表名 -I 链名 2 -s -d -j  增加第二行\niptables -t 表名 -A 链名 -s -d -j  在尾行增加\niptables -t filter -I INPUT -s 192.168.2.35 -j ACCEPT\n  删 -D\niptables -t 表名 -D 链名 第几行\niptables -D INPUT 2\n  改 -R\n用-R，但是不建议使用，一般使用先删除，再增加的方式，如果非要用，必须要指定原来的条件\n如果执行\niptables -R INPUT 3 -j REJECT 则会变成这样，把源地址和目的地址都变成0.0.0.0/0 因此要慎用\n使用 -P 修改链路的默认规则\niptables -P INPUT ACCEPT \n  保存规则\niptables-save \u0026gt;/etc/sysconfig/iptables\niptables-restore \u0026lt; /etc/sysconfig/iptables\n  iptables 条件匹配   源地址条件匹配\niptables -I INPUT -s 192.168.129.2,192.168.3.13 -j ACCEPT 用，配置多个源地址\n源地址取反操作 ！\niptables -I INPUT 2 ! -s 192.168.2.244 -j ACCEPT 这个语句的意思是，如果source ip不是192.168.23.2则往下执行。而不是说非192.168.2.2\n  协议匹配\n-p\niptables -I input -s 192.168.2.244 -p tcp -j REJECT 只限制住tcp，但是icmp不限制\n  网卡匹配：-i -o\n-i 只能用在 PREROUTING，INPUT，FORWARD\n-o 只能用在 OUTPUT，POSTROUTING，FORWARD\niptables -I input -s 192.168.2.244 -i eth0 -p icmp -j REJECT  对 网卡 限制ping\n  iptables条件匹配扩展  tcp模块  ​ -p tcp -m tcp \u0026ndash;dport |\u0026ndash;sport 可以指定连续的端口，也可以指定单个端口，如果不指定-m tcp，则用-p的协议作为模块\n#示列 iptables -I INPUT -p tcp -m tcp --dport :25 -j ACCEPT iptables -I INPUT -p tcp --dport 30: -j ACCEPT iptables -I INPUT -p tcp --dport 2000:4000 -j ACCEPT ## 源地址不是22端口的接受 iptables -I INPUT -p tcp -m tcp ! --sport 22 -j ACCEPT #结果如下： 65 3868 ACCEPT tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp spt:!22 0 0 ACCEPT tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpts:2000:4000 522 26100 ACCEPT tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpts:30:65535 154 11068 ACCEPT tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpts:0:25    multiport模块\n常用模块，指定离散的多个ip端口，用,隔开\t注意是\u0026ndash;dports和\u0026ndash;sports\n[root@local-master ~]# iptables -I INPUT -p tcp -m multiport --dports 8890,8888 -j DROP [root@local-master ~]# iptables -vnL Chain INPUT (policy ACCEPT 114 packets, 8028 bytes) pkts bytes target prot opt in out source destination 0 0 DROP tcp -- * * 0.0.0.0/0 0.0.0.0/0 multiport dports 8890,8888    iprange模块\n--src-range 指定连续的源地址范围\n--dst-range 指定连续的目的地址范围\n iptables -I INPUT -m iprange --src-range 192.168.2.1-192.168.2.244 -j ACCEPT\n  string 模块 ，过滤关键字\n  time 模块 ， 指定时间放行\n  connlimt模块，限制ip的链接数量\n  limit模块\n--limit-burst 类似“令牌桶”算法，用于指定令牌桶的最大上限\n--limit ，类似\u0026quot;令牌桶\u0026quot;算法，指定令牌桶的生产新令牌的频率，可用时间为second，minute，hour，day\n###例 ##限制ping的速率 // 最大令牌为3个，每一分钟生成30个令牌，相当于每两秒钟生成一个令牌，及每两秒钟放行一个 iptables -I INPUT -p icmp -m limit --limit-burst 3 --limit 30/minute -j ACCEPT //如果iptables 的默认规则为ACCEPT，需要用REJECT做往下匹配限制 iptables -A INPUT -p icmp -j REJECT    ","id":22,"section":"posts","summary":"四表五链 四表 五链 filter\u0026mdash; 过滤数据包 nat \u0026mdash; 用于网络地址转换（IP、端口） raw \u0026mdash; 决定数据包是否被状态跟踪机制处理 mangle \u0026mdash;修改数据包的服务类型、TT","tags":["iptables","linux"],"title":"Iptables(一)","uri":"https://zhangshunping.github.io/2020/05/iptables-1/","year":"2020"},{"content":"[TOC]\nstate 模块   对于state模块的连接而言，\u0026ldquo;连接\u0026quot;其中的报文可以分为5种状态，报文状态可以为NEW、ESTABLISHED、RELATED、INVALID、UNTRACKED\n  NEW: 新连接得第一个包为new\n  ESTABLISHED: NEW状态包后面的包的状态理解为ESTABLISHED，表示已建立链接\n  RELATED：有些报文返回是需要多个进程直接相互配合进项，比如FTP服务命令进程和数据进程是有关系的，因此数据链接的报文可能就是RELATED，因为他是由命令进程控制的。\n  INVALID: 报文没有被识别，包没有状态，包的状态INVALID，可以主动屏蔽INVALID的报文\n  UNTRACED:报文没有被追踪，表示无法找到相关的链接\n    因此：客户端，怎样判断这些报文是为了回应我们之前发出的报文，还是主动向我们发送的报文呢？\niptables -t fiflter -I INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT  则表示，客户端不允许被主动发起报文。\n黑白名单   方式一：（不推荐）设置链路的默认规则为DROP\\\n##1、 设置链路的默认规则为Drop iptables -P INPUT DROP ##2、放行 iptables -I INPUT --dport 80 -j ACCEPT ## 问题 如果不小心执行iptahles -F 的会导致所有请求无法访问    方式二：（推荐）设置链路的默认规则为ACCEPT，在末尾设置DROP\niptables -P INPUT -j ACCEPT iptables -I INPUT --dport 80 -j ACCEPT iptables -A INPUT -j DROP    自定义链路 创建自定义链路 #1. -N 创建自定义表 iptables -t filter -N IN_WEB #2. 常看自定义表 iptables --line -vnL IN_WEB ##显示如下 ： 0 references 表示没有被引用  #3、在默认链路上引用IN_WEB ## 在INPUT链路上插入规则，表示访问80的端口，跳转到IN_WEB自定义链路上 iptables -I INPUT -p tcp --dport 80 -j IN_WEB [root@k8s-master01 ~]# iptables -vnL INPUT Chain INPUT (policy ACCEPT 273 packets, 24366 bytes) pkts bytes target prot opt in out source destination 2 120 IN_WEB tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:80 8339 783K KUBE-EXTERNAL-SERVICES all -- * * 0.0.0.0/0 0.0.0.0/0 ctstate NEW /* kubernetes externally-visible service portals */ 828K 177M KUBE-FIREWALL all -- * * 0.0.0.0/0 0.0.0.0/0 [root@k8s-master01 ~]# iptables -vnL IN_WEB Chain IN_WEB (1 references) ## 这里的 references为1 表示被引用一次 pkts bytes target prot opt in out source destination 2 120 REJECT tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:80 reject-with icmp-port-unreachable [root@k8s-master01 ~]# curl 127.0.0.1 curl: (7) Failed connect to 127.0.0.1:80; 拒绝连接  重命名自定义链路 -E [root@k8s-master01 ~]# iptables -E IN_WEB IN_WEB2 [root@k8s-master01 ~]# iptables -vnL INPUT Chain INPUT (policy ACCEPT 63 packets, 5259 bytes) pkts bytes target prot opt in out source destination 4 240 IN_WEB2 tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:80 8491 798K KUBE-EXTERNAL-SERVICES all -- * * 0.0.0.0/0 0.0.0.0/0 ctstate NEW /* kubernetes externally-visible service portals */ 828K 177M KUBE-FIREWALL all -- * * 0.0.0.0/0 0.0.0.0/0 [root@k8s-master01 ~]# iptables -vnL IN_WEB2 Chain IN_WEB2 (1 references) pkts bytes target prot opt in out source destination 4 240 REJECT tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:80 reject-with icmp-port-unreachable  删除自定义链路 -X ##删除链路需要满足 ## 1、自定义链没有被任何默认链引用，即自定义链的引用计数为0。 ##2、自定义链中没有任何规则，即自定义链为空。 [root@k8s-master01 ~]# iptables -X IN_WEB2 iptables: Too many links. [root@k8s-master01 ~]# iptables -D INPUT 1 ## 清空引用链路 [root@k8s-master01 ~]# iptables -X IN_WEB2 iptables: Directory not empty. [root@k8s-master01 ~]# iptables -F IN_WEB2 ## 清空规则 [root@k8s-master01 ~]# iptables -X IN_WEB2  Iptables网络防火墙 1、需求： 主机A访问主机C，A和C不在同一个网络，借助主机B的iptables路由规则 ##主机A,添加路由，去往192.168.181.0（外网）的报文的下一跳为主机B route add -net 192.168.181.0/24 gw 192.168.2.17 ##主机C，添加路由，去往192.168.2.0(外网的报文)的下一跳为主机B route add -net 192.168.2.0/24 gw 192.168.2.17 ## 主机B 永久开启路由转发 vim /etc/sysctl.conf net.ipv4.ip_forward ## 临时开启 echo 1 \u0026gt;/proc/sys/net/ipv4/ip_forward ##主机B设置黑名单 iptables -t filter -P FORWARD ACCEPT iptables -A FORWARD DROP iptables -E FORWARD_TEST iptables -I FORWARD_TEST -s 192.168.2.0/24 -j ACCEPT iptables -I FORWARD_TEST -s 192.168.181.0/24 -j ACCEPT iptables -I FORWARD -j FORWARD_TEST  2、通过snat，dnat转发。实现公网端口暴露和私网上网  模型如下，主机A为公网客户都安，主机B为公司iptables linux，主机C为内网主机    操作步骤\n#主机B需开启路由 vim /etc/sysctl.conf net.ipv4.ip_forward # 添加snat配置(允许私网通过B主机的ip，访问A) iptables -t nat -I POSTROUTING -s 192.168.181.0/24 -j SNAT --to-source=192.168.2.17 # 添加dnat配置（A访问B的122端口，即可访问C主机的22端口 iptables -t dnat -I PREROUTING -d 192.168.2.18 -p tcp --dport 122 --to-destination=192.168.181.129:22 ##同时检查 filter表上是否有限制操作 iptables -t filter -vnL FORWARD ## 主机c需要添加路由 route add default gw 192.168.181.128    dnat和snat转发，普通路由转发的区别   共同点\n 都需要linux 开启路由转发功能 私有主机，及C主机都需要配置下一跳到B主机    不同点\n  dnat和snat 是描述的私网和公网的情况，因此主机A配置route 到B是广播不可达的；而但单纯的路由转发一般在同一二层网络中（比如vmware开启三台主机）\n  dnat和snat 完成的地址转换，经过B主机之后，ip的地址都会变成B主机的地址；而单纯的路由转发，ip地址不变\n  MASQUSEADE  可以把MASQUERADE理解为动态的、自动化的SNAT，如果没有动态SNAT的需求，没有必要使用MASQUERADE，因为SNAT更加高效。  iptables -t nat -I POSTROUTING -s 192.168.181.0/24 -o ens32 -j MASQUSEADE  REDIRECT动作可以在本机上进行端口映射   比如，将本机的80端口映射到本机的8080端口上\niptables -t nat -A PREROUTING -p tcp --dport 80 -j REDIRECT --to-ports 8080\n经过上述规则映射后，当别的机器访问本机的80端口时，报文会被重定向到本机的8080端口上。\nREDIRECT规则只能定义在PREROUTING链或者OUTPUT链中。\n  iptables 注意点：  针对相同服务的时候，严格的规则应该放在前面 当规则中有多个匹配条件时，条件之间默认存在\u0026quot;与\u0026quot;的关系 在不考虑1的情况下，应该将更容易被匹配到的规则放置在前面，通过iptables -vnL 查看（pkts计数） 当IPTABLES所在主机作为网络防火 墙时，在配置规则时，应着重考虑方向性，双向都要考虑，从外到内，从内到外 设置防护墙白名单的时候，默认规则应为ACCEPT，在某行增加DROP动作，防止iptables -F 导致不可登陆  ","id":23,"section":"posts","summary":"[TOC] state 模块 对于state模块的连接而言，\u0026ldquo;连接\u0026quot;其中的报文可以分为5种状态，报文状态可以为NEW、ESTABLISHED","tags":["iptables","linux"],"title":"Iptables(二)","uri":"https://zhangshunping.github.io/2020/05/iptables-2/","year":"2020"},{"content":"channle 都是应用于两个go程，一个读，一个写  channel 具有阻塞的作用，类似go程之间对同步资源进行锁机制 具有数据传递的功能  一、无缓冲channel \u0026mdash;-（同步） 无缓冲channel 定义  var ch =make(chan string ) 或者ch :=make(chan string)  无缓冲channel的说明  ch \u0026lt;- \u0026ldquo;hehe\u0026rdquo; 写数据，数据没有被在读，则为阻塞 \u0026lt;-ch 读数据，数据没有没有在写， 则为阻塞 len(ch) ：channel中未读取的元素个数，cap(ch):channel中通道的容量 。 都是零  二、有缓冲channel \u0026mdash;\u0026mdash;（异步） 有缓冲channel定义  var ch =make(chan string ,3) 或者ch :=make(chan string ,3) 当存储的元素个数超过了cap(ch) 才会阻塞  三、关闭channel   close(ch) ，不再向对端发送数据\n  判断chan是否关闭 ，\nif num,ok:=\u0026lt;-chan; ok ==true{ fmt.Println(\u0026quot;已经关闭\u0026quot;) } // 无缓冲 关闭channle，读端 ok为false，num为0 // 如果channel已经关闭 ，写端出现panic send on closed channel // 如果channle有缓冲，当len(ch) 的值没有超过cap(ch),那么close(ch)是阻塞的 // 如果channle无缓冲，当ch中存在元素，close（ch）也是阻塞的 func main() { ch:=make(chan int) go func() { for i:=0;i\u0026lt;=100;i++{ ch \u0026lt;-i } close(ch) //这里close其实是阻塞的 }() go func() { for{ if num,ok:=\u0026lt;-ch;ok{ fmt.Print(num) close(ch) }else{ fmt.Println(\u0026quot;jieshu\u0026quot;,num) } } }()    四、单向channel   双向channel ch:=make(chan string )\n  单项写channel var readCh chan\u0026lt;- int\n  单向读channel var writech \u0026lt;-chan\n  单向channel和双向channel转换  双向转单向 readch = ch 单向不可以转双向  传参：传【引用】 Channel通信的时候，要保证channle不能在同一个go程之中\n五、消费者，生产者模型  解耦（降低消费者和生产者直接的耦合度） 并发（消费者，生产者消费不对等，能保持正常的通信） 缓存（生产者和消费者处理不一致时，暂存数据）  生产者，消费者实现：  - 有缓冲的channel ：异步同行 - 无缓冲的channel：同步通信  package main import \u0026quot;fmt\u0026quot; type Orderinfo struct { id int price int } func producer(ch chan\u0026lt;- Orderinfo) { for i:=0;i\u0026lt;=1000;i++{ ch\u0026lt;-Orderinfo{id:i+1,price: 10} } close(ch) } func consumer(ch \u0026lt;-chan Orderinfo) { for order:=range(ch){ fmt.Println(\u0026quot;订单价格：\u0026quot;,order.price,\u0026quot;订单编号：\u0026quot;,order.id) } } func main() { ch :=make(chan Orderinfo) go producer(ch) consumer(ch) }  六、定时器: 三种定义方法 time.sleep() ; time.NewTimer() ;time.After() package main import ( \u0026quot;fmt\u0026quot; \u0026quot;time\u0026quot; ) func main() { //time.sleep定时 fmt.Println(\u0026quot;当前时间\u0026quot;,time.Now()) time.Sleep(time.Second*1) fmt.Println(\u0026quot;time.sleep定时当前时间\u0026quot;,time.Now()) //time.Newtimer // 创建定时器 t:=time.NewTimer(time.Second*1) now:=\u0026lt;-t.C fmt.Println(\u0026quot;time.NewTimer阻塞 t.C管道读取系统当前时间为：\u0026quot;,now) //time.After 返回一个管道 now=\u0026lt;-time.After(time.Second) fmt.Println(\u0026quot;time.After，系统chan写满，则返回当前时间：\u0026quot;,now) }  七、select 语句 select (退出for 循环中的select ，用return 或者goto的方式)\nfunc main() { ch:=make(chan int) quit:=make(chan bool) fmt.Println(\u0026lt;-time.After(time.Second)) go func() { for{ select { case num:=\u0026lt;-ch: fmt.Println(\u0026quot;nume\u0026quot;,num) case \u0026lt;-time.After(time.Second*3): fmt.Println(\u0026quot;超时\u0026quot;) quit\u0026lt;-true //return goto loop } } loop: fmt.Println(\u0026quot;for end\u0026quot;) }() for i:=0;i\u0026lt;4;i++{ ch\u0026lt;-i time.Sleep(time.Second) } \u0026lt;-quit }  八、锁 golang死锁模型：  1、单go程自己死锁： channel至少在两个go程中使用，否则思索 2、go程间channel访问顺序：使用channle一端写（读），要保证另一一段读（写），同时有机会执行，否则思索 3、多go程，多channel交叉死锁 4、在go语言中，尽量不要将互斥锁和channel一起使用，会造成隐形死锁  golang 锁  1.channel 定义互斥锁  //定义个全局channel var ch =make(chan string) func Printer(s string){ for _,s1:=range(s){ fmt.Print(string(s1)) \u0026lt;-time.After(time.Microsecond*500000) } } func main() { //go程里结束，向ch中传递 go func() { Printer(\u0026quot;hello\u0026quot;) ch\u0026lt;-\u0026quot;ok\u0026quot; close(ch) }() // 主go程在阻塞等待 \u0026lt;-ch Printer(\u0026quot;world\u0026quot;) }    2.用锁定义互斥锁（这种锁，我们一般称为:\u0026ndash;\u0026gt;建议锁)\nvar mutex sync.Mutex //定义锁 //对共享资源枷锁 mutex.Lock() fmt.Pirnt('hello') muter.Unlock()    3.读写锁: 读时共享，写时独占（写比读优先级要高）\n var rwmutex2 sync.RWMutex 定了一把锁，这个锁带有读和写的属性  var rwmutex2 sync.RWMutex var I int func reader2(i int){ for{ // 读锁，并发共享 rwmutex2.RLock() a:=I rwmutex2.RUnlock() fmt.Printf(\u0026quot;=======第%d个读进程,读取到%d\\n\u0026quot;,i,a) time.Sleep(time.Second) } } func writer2(a int){ for{ num:=rand.Intn(100) // 写锁，独占 rwmutex2.Lock() I=num fmt.Printf(\u0026quot;第%dth写go程,写数据:%d \\n\u0026quot; ,a,num) time.Sleep(time.Second*2) rwmutex2.Unlock() } } func main(){ for i:=0;i\u0026lt;=3;i++{ go reader2(i) } for i:=0;i\u0026lt;=3;i++{ go writer2(i) } for{ ; } }   channel 无法完成共享读，因此需要对读完成共享数据读的时候，使用读写锁    条件变量 type Cond struct   适用生产者消费者模型：本身不是锁，经常要与锁结合使用,他是一个结构体\n  作用：\n 加锁 访问公共区域（比如带缓冲channle） 解锁 唤醒阻塞在条件变量上对端    条件变量Cond常见方法：\n  wait（） 函数 使用场景\n 阻塞等待条件变量满足 释放已掌握的互斥锁相当于 cond.L.Unlock() ;注意：两步分为原子操作 \u0026lt;\u0026mdash;-及不可以分开 当被唤醒，Wati（）函数返回时候，解决阻塞并重新获取互斥锁。相当于cond.L.Lock()    cond.Single() 唤醒对端\n  cond.Broadcast()惊群唤醒\n    创建条件变量流程\n//1.创建条件变量 var cond sync.Cond //2.创建条件变量指定锁 cond.L=new(sync.Mutex) //3. 给公共区加锁 cond.L.Lock() //4.判断是否达到阻塞条件 // //4.1 producer 端判断条件 for len(ch)==cap(ch){ cond.Wait() //1.阻塞2.解锁 3.加锁 } //4.2 consumer端判断 for len(ch)==0{ cond.Wait() } //5.访问公共区，读写，打印 //6.对公共区条件变量解锁 cond.L.Unlock() //7.唤醒阻塞在条件变量对端 cond.Single()  【条件变量示例】\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;sync\u0026quot; \u0026quot;time\u0026quot; ) //1.创建条件变量 var cond sync.Cond func producer2(product chan\u0026lt;- string,index int) { //3.使用条件变量给公共区加锁 cond.L.Lock() //4. 判断是否已经填满缓冲区，如果填满则wait() for len(product)==cap(product){ cond.Wait() } //5.执行写操作 str:=fmt.Sprintf(\u0026quot;produce%d\u0026quot;,index) product\u0026lt;-str fmt.Printf(\u0026quot;这是第%d个producer进程,produce %q \\n\u0026quot;,index,str) time.Sleep(time.Millisecond*200) // 6.解锁 cond.L.Unlock() //7.唤醒阻塞的对端 cond.Signal() } func consumer2(product \u0026lt;-chan string,index int) { for{ cond.L.Lock() for len(product)==0{ cond.Wait() } str\t:=\u0026lt;-product fmt.Printf(\u0026quot;--------这是第%d个consumer进程,consum %s \\n\u0026quot;,index,str) cond.L.Unlock() cond.Signal() } } func main() { product:=make(chan string,3) // 2.指定条件变量用的锁为互斥锁 cond.L=new(sync.Mutex) for i:=1;i\u0026lt;=1000;i++{ go producer2(product,i) } for i:=1;i\u0026lt;=3;i++{ go consumer2(product,i) } for{ ; } }    ","id":24,"section":"posts","summary":"channle 都是应用于两个go程，一个读，一个写 channel 具有阻塞的作用，类似go程之间对同步资源进行锁机制 具有数据传递的功能 一、无缓冲channel \u0026mda","tags":["golang-channel"],"title":"Golang-Channel","uri":"https://zhangshunping.github.io/2020/05/golang-channel/","year":"2020"},{"content":" rysnc 常用选项 rsync常用选项 -a 包含-rtplgoD -r 同步目录时要加上，类似cp时的-r选项 -v 同步时显示一些信息，让我们知道同步的过程 -l 小写l保留软连接，例如A机器上面的文件有软连接所指向的文件，同步到B机器时同样也保留软连接。 -L 大写L加上该选项后，同步软链接时会把源文件给同步 -p 小写p保持文件的权限属性 -o 保持文件的属主 -g 保持文件的属组 -D 保持设备文件信息 -t 保持文件的时间属性 --delete 删除DEST中SRC没有的文件 --exclude 过滤指定文件，如--exclude “logs”会把文件名包含logs的文件或者目录过滤掉，不同步 -P 大写P显示同步过程，比如速率，比-v更加详细 -u 加上该选项后，如果DEST中的文件比SRC新，则不同步 -z z表示zip传输时压缩，传输到目标点后自动就解压了，只是在传输前或传输过程中减少网络资源带宽。 --existing ：要求只更新目标端已存在的文件，目标端还不存在的文件不传输。注意，使用相对路径时如果上层目录不存在也不会传输。 --ignore-existing：要求只更新目标端不存在的文件。和\u0026quot;--existing\u0026quot;结合使用有特殊功能，见下文示例。 --delete ：以SRC为主，对DEST进行同步。多则删之，少则补之。注意\u0026quot;--delete\u0026quot;是在接收端执行的，所以它是在 ：exclude/include规则生效之后才执行的。 -d --dirs ：以不递归的方式拷贝目录本身。默认递归时，如果源为\u0026quot;dir1/file1\u0026quot;，则不会拷贝dir1目录，使用该选项将拷贝dir1但不拷贝file1。 -u --update ：仅在源mtime比目标已存在文件的mtime新时才拷贝。注意，该选项是接收端判断的，不会影响删除行为。  常用命令 拷贝复制   ## 将/home/bridg.war 增量同步到/tmp/下 命名为bridge2.war rsync -Pzav /home/bridge.war /tmp/bridge2.war ## 将/home/bridg.war 增量同步到/tmp/ rsync -Pzav /home/bridge.war /tmp/ ## --delete的使用 ，删除DEST中SRC没有的文件 ##注意：src目录没有的，dst目录也会被删掉 [root@k8s-master01 /tmp/tmp.V8GRsfNIeG]# tree ./ ./ ├── t1 │ ├── a │ ├── b │ └── c └── t2 ├── a └── c [root@k8s-master01 /tmp/tmp.V8GRsfNIeG]# rsync -Pzav --delete t2/ t1/ [root@k8s-master01 /tmp/tmp.V8GRsfNIeG]# tree ./ ./ ├── t1 │ ├── a │ └── c └── t2 ├── a └── c    过滤 --exclude\n[root@k8s-master01 /tmp/tmp.V8GRsfNIeG]# ll t2/ 总用量 0 -rw-r--r-- 1 root root 0 5月 21 11:35 a -rw-r--r-- 1 root root 0 5月 21 11:35 c [root@k8s-master01 /tmp/tmp.V8GRsfNIeG]# ll t1/ 总用量 0 -rw-r--r-- 1 root root 0 5月 21 11:35 a -rw-r--r-- 1 root root 0 5月 21 11:35 c -rw-r--r-- 1 root root 0 5月 21 11:41 jss.js -rw-r--r-- 1 root root 0 5月 21 11:41 readme.txt [root@k8s-master01 /tmp/tmp.V8GRsfNIeG]# rsync -av --exclude \u0026quot;*.js\u0026quot; --exclude \u0026quot;*.txt\u0026quot; t1/ t2/ ## 显示结果js和txt并未拷贝 [root@k8s-master01 /tmp/tmp.V8GRsfNIeG]# ll t2/ 总用量 0 -rw-r--r-- 1 root root 0 5月 21 11:35 a -rw-r--r-- 1 root root 0 5月 21 11:35 c [root@k8s-master01 /tmp/tmp.V8GRsfNIeG]# ll t1/ 总用量 0 -rw-r--r-- 1 root root 0 5月 21 11:35 a -rw-r--r-- 1 root root 0 5月 21 11:35 c -rw-r--r-- 1 root root 0 5月 21 11:41 jss.js -rw-r--r-- 1 root root 0 5月 21 11:41 readme.txt    使用sshpasss, ssh 远程同步\nsshpass -p 123123 rsync -av -e \u0026quot;ssh -p 22\u0026quot; root@127.0.0.1:/tmp/t2/ /tmp/t1/    ​\n  使用rsync 删除大文件\n## 在遇到很大的目录时候，使用rm去删除，会出现报错 ##先创建一个空目录 mkdir /tmp/empty/ ## 清空目标目录 # rsync --delete-before -avH --progress --stats /tmp/empty/ /var/spool/postfix/maildrop rsync --delete -rlptD /tmp/empty/ /var/spool/postfix/maildrop/ ##选项说明： -delete-before 接收者在传输之前进行删除操作 –progress 在传输时显示传输过程 -a 归档模式，表示以递归方式传输文件，并保持所有文件属性 -H 保持硬连接的文件 -v 详细输出模式 –stats 给出某些文件的传输状态 ## 注意： ## 不过在使用上面的命令进行清理时，存在一个问题，清空后，目标目录的权限会和源目录的权限一样。如：/tmp/empty是root：root，而maildrop之前是postfix：postdrop ，执行之后也会maildrop目录的权限也会变成root：root 。由于-a权限是-rlptogD几个参数的集合，所以可以将og（owner:group）两个参数去掉。清空时自动保持之前的目录权限，如下： rsync --delete -rlptD /tmp/empty/ /var/spool/postfix/maildrop/    ","id":25,"section":"posts","summary":"rysnc 常用选项 rsync常用选项 -a 包含-rtplgoD -r 同步目录时要加上，类似cp时的-r选项 -v 同步时显示一些信息，让我们知道同步的过程 -l 小写l","tags":["Linux","rync"],"title":"Rsync","uri":"https://zhangshunping.github.io/2020/05/rsync/","year":"2020"},{"content":" curl -o /dev/null -s -w %{time_namelookup}---%{time_connect}---%{time_starttransfer}---%{time_total}---%{speed_download}\u0026quot;\\n\u0026quot;  查看链接\n[root@pre-host-work02 ~]# curl -o /dev/null -s -w %{time_namelookup}---%{time_connect}---%{time_starttransfer}---%{time_total}---%{speed_download}\u0026quot;\\n\u0026quot; https://test-newweb.educoder.net/api/myshixuns/training_task_status.json\r0.004---31.731---34.086---34.086---1.000\r ","id":26,"section":"posts","summary":"curl -o /dev/null -s -w %{time_namelookup}---%{time_connect}---%{time_starttransfer}---%{time_total}---%{speed_download}\u0026quot;\\n\u0026quot; 查看链接 [root@pre-host-work02 ~]# curl -o /dev/null -s -w %{time_namelookup}---%{time_connect}---%{time_starttransfer}---%{time_total}---%{speed_download}\u0026quot;\\n\u0026quot; https://test-newweb.educoder.net/api/myshixuns/training_task_status.json 0.004---31.731---34.086---34.086---1.000","tags":null,"title":"","uri":"https://zhangshunping.github.io/1/01/curl/","year":"0001"},{"content":"表定义 type Like struct {\rID int `gorm:\u0026quot;primary_key\u0026quot;`\rIp string `gorm:\u0026quot;type:varchar(20);not null;index:ip_idx\u0026quot;`\rUa string `gorm:\u0026quot;type:varchar(256);not null;\u0026quot;`\rTitle string `gorm:\u0026quot;type:varchar(128);not null;index:title_idx\u0026quot;`\rHash uint64 `gorm:\u0026quot;unique_index:hash_idx;\u0026quot;`\rCreatedAt time.Time\r}\r 创建表 if !db.HasTable(\u0026amp;Like{}) {\rif err := db.Set(\u0026quot;gorm:table_options\u0026quot;, \u0026quot;ENGINE=InnoDB DEFAULT CHARSET=utf8\u0026quot;).CreateTable(\u0026amp;Like{}).Error; err != nil {\rpanic(err)\r}\r}\r 增 like := \u0026amp;Like{\rIp: ip,\rUa: ua,\rTitle: title,\rHash: murmur3.Sum64([]byte(strings.Join([]string{ip, ua, title}, \u0026quot;-\u0026quot;))) \u0026gt;\u0026gt; 1,\rCreatedAt: time.Now(),\r}\rif err := db.Create(like).Error; err != nil {\rreturn err\r}\r 先构造已给对象，直接调用 db.Create() 就可以插入一条记录了\n删除 if err := db.Where(\u0026amp;Like{Hash: hash}).Delete(Like{}).Error; err != nil {\rreturn err\r}\r 查询 var count int\rerr := db.Model(\u0026amp;Like{}).Where(\u0026amp;Like{Ip: ip, Ua: ua, Title: title}).Count(\u0026amp;count).Error\rif err != nil {\rreturn false, err\r}\r 修改 db.Model(\u0026amp;user).Update(\u0026quot;name\u0026quot;, \u0026quot;hello\u0026quot;)\rdb.Model(\u0026amp;user).Updates(User{Name: \u0026quot;hello\u0026quot;, Age: 18})\rdb.Model(\u0026amp;user).Updates(User{Name: \u0026quot;\u0026quot;, Age: 0, Actived: false}) // nothing update\r ","id":27,"section":"posts","summary":"表定义 type Like struct { ID int `gorm:\u0026quot;primary_key\u0026quot;` Ip string `gorm:\u0026quot;type:varchar(20);not null;index:ip_idx\u0026quot;` Ua string `gorm:\u0026quot;type:varchar(256);not null;\u0026quot;` Title string `gorm:\u0026quot;type:varchar(128);not null;index:title_idx\u0026quot;` Hash uint64 `gorm:\u0026quot;unique_index:hash_idx;\u0026quot;` CreatedAt time.Time } 创建表 if !db.HasTable(\u0026amp;Like{}) { if err := db.Set(\u0026quot;gorm:table_options\u0026quot;, \u0026quot;ENGINE=InnoDB DEFAULT CHARSET=utf8\u0026quot;).CreateTable(\u0026amp;Like{}).Error; err != nil { panic(err) } } 增 like := \u0026amp;Like{ Ip: ip, Ua: ua, Title: title, Hash: murmur3.Sum64([]byte(strings.Join([]string{ip, ua, title}, \u0026quot;-\u0026quot;))) \u0026gt;\u0026gt; 1, CreatedAt: time.Now(), } if err","tags":null,"title":"","uri":"https://zhangshunping.github.io/1/01/gorm/","year":"0001"},{"content":"[TOC]\nGrpc Prototbuf 协议 protobuf是由Google开发的一种数据序列化协议，可以把它想象成是XML或JSON格式，但更小，更快更简洁。而且一次定义，可生成多种语言的代码。\n一、golang Protocol 环境配置   Protocol 命令下载\n  安装protoc-gen-go.exe go get -u github.com/golang/protobuf/protoc-gen-go\n  golang 安装Protoful插件\n  二、编译生成中间文件 Prod.proto 文件\nsyntax = \u0026quot;proto3\u0026quot;;\rpackage service;\rmessage HelloWorldRequest {\rstring greeting = 1;\r}\rmessage HelloWorldResponse {\rstring reply = 1;\r}\r 用protoc 编译:\n## 胜场Prod.pd.go 文件 protoc.exe --go_out=./services/ pdfiles/Prod.proto  三、编译生成Grpc中间文件\ncd pdfiles; protoc.exe --go_out=plugins=grpc:../services/ Prod.proto\r ","id":28,"section":"posts","summary":"[TOC] Grpc Prototbuf 协议 protobuf是由Google开发的一种数据序列化协议，可以把它想象成是XML或JSON格式，但更小，更快更简洁。而且一次定义，","tags":null,"title":"","uri":"https://zhangshunping.github.io/1/01/grpc%E4%B8%80/","year":"0001"},{"content":"[TOC]\nIstio版本 \u0026gt;=1.5 安装 ## kubernetes上安装istioctl\r##1. istio安装 profile为demo的istio 集群\ristioctl manifest apply --set profile=demo\r##2. 修改ingeress 如果没有loadbalance 则修改为 nodeport\rkubectl patch svc istio-ingressgateway -nistio-system -p '{\u0026quot;spec\u0026quot;:{\u0026quot;type\u0026quot;:\u0026quot;NodePort\u0026quot;}}'\r##3. kubectl get all -nistio-system\r##4. 卸载istio\ristioctl manifest generate --set profile=demo | kubectl delete -f -\r Istioctl 使用 profile istioctl profile list\ristioctl profile dump default \u0026gt;default.yaml ## 一般对端口修改的时候可以使用这样的方式dump下来,修改完之后执行\r## istioctl manifest apply -f defaul.yaml  Inject 解释：Inject Envoy sidecar into Kubernetes pod resources（针对service ，secrete,configmap 资源不起作用）。\n 主动注入envoy sidecar  istioctl kube-inject -f deploynginx.yaml |kubectl apply -f - -n default\r  自动注入  ## 给命名空间打入标签 istio-injection=enable, 则在此命令空间下的资源，都会被自动注入sidecar。\ristioctl label ns jixu istio-injection=enabled\r   istio-proxy 运行原理\n  ​\n如图： nginx Pod中会同时启动三个容器分别是：\n  initContainer\n 执行 istio-iptables脚本创建istio的四个链路，创建完之后退出    Istio-Proxy(istio-pilot,envoy)\n[root@k8s-slave01 ~]# kubectl exec -it nginx-64759d697c-mgxqb -c istio-proxy -- ps -ef\rUID PID PPID C STIME TTY TIME CMD\ristio-p+ 1 0 0 08:12 ? 00:00:00 /usr/local/bin/pilot-agent proxy\ristio-p+ 19 1 0 08:12 ? 00:00:01 /usr/local/bin/envoy -c etc/isti\ristio-p+ 51 0 0 08:15 pts/0 00:00:00 ps -ef\r  运行istio\u0026ndash;agent,根据pilot-discovery 同步数据（pilot-discovery是运行在k8s istio-system下的istiod服务端，它watch kube-apiserver的变化，当用户对k8s资源进行操作时，istiod会同步 api-server的资源，istio-proxy中的istio-agent容器sync istio-disvoery的配置规则，istio-agent将配置文件策略和行为告诉envoy，从而实现对流量控制和行为管理） envoy 是一个轻量级的代理服务。    nginx Container\n web业务容器    注意小技巧：initContainer和istio-proxy用的时同一个镜像，而initContainer执行完istio-iptables脚本之后，就退出，Iistio-agent一直运行是因为,\n镜像中的entypoint是\n \u0026quot;Entrypoint\u0026quot;: [\r\u0026quot;/usr/local/bin/pilot-agent\u0026quot;\r initContainers 中使用的command\nenableServiceLinks: true\rinitContainers:\r- Commnads:\r- istio-iptables\r k8s 中的command覆盖dockerfile中entryPoint， 不过istio 1.65不再使用这种方式，initContainer直接是args，可能对命令做了封装。\nIstio Traffic Management 流量管理简单点就是：使用策略控制流量的流向和大小。\n同时，业务代码只需要关注业务，超时和重连等机制直接交给istio流量管理即可。\nIstio Architecture：（istio 1.5之后）\n 数据面流量：业务之间调用的流量 控制面流量：istio各组件之间配置和控制网格行为的流量  Istio virtual service   概念： virtual service 是一个抽象概念，简单点可以认为是一个virtual service 是一个类似nginx.conf中location 这样的片段文件\n  配置：\n hosts filed： 可配置为 短域名（servicename),长域名(servicename.namesapce.default.local.cluster) 和ingress controller    ","id":29,"section":"posts","summary":"[TOC] Istio版本 \u0026gt;=1.5 安装 ## kubernetes上安装istioctl ##1. istio安装 profile为demo的istio 集群 istioctl manifest apply --set profile=demo ##2. 修改i","tags":null,"title":"","uri":"https://zhangshunping.github.io/1/01/istio-1/","year":"0001"},{"content":"下载 kubernetes  windows 保持软连接  git -c core.symlinks=true clone https://github.com/kubernetes/kubernetes -b release-1.16\r ","id":30,"section":"posts","summary":"下载 kubernetes windows 保持软连接 git -c core.symlinks=true clone https://github.com/kubernetes/kubernetes -b release-1.16","tags":null,"title":"","uri":"https://zhangshunping.github.io/1/01/kubelet/","year":"0001"},{"content":"","id":31,"section":"posts","summary":"","tags":null,"title":"","uri":"https://zhangshunping.github.io/1/01/altermanager/","year":"0001"},{"content":"[toc]\npromethues +altermanager 告警流程  如下图所示：通过prometheus定义的AlterRule，prometheus会周期性的对alterRules进行计算，当满足告警触发条件后，就会想altermanger发送告警信息  prometheus中alterRuler配置说明 告警规则配置文件 rules.yml\ngroups:\r- name: example\rrules:\r- alert: HighErrorRate\rexpr: job:request_latency_seconds:mean5m{job=\u0026quot;myjob\u0026quot;} \u0026gt; 0.5\rfor: 10m\rlabels:\rseverity: page\rannotations:\rsummary: High request latency\rdescription: description info\r 我们可以将一组相关规则定义在一个group下，在每一个group下我们可以定义多个告警规则（rule），一条告警规则主要由以下几个部分组成:\n alter : 告警规则名称 expr: 基于PromQL表达式告警触发条件，用于计算是否有时间序列满足该条件。 for: 可选参数，等待时间，用于表示只有当触发条件持续一段时间后才发送告警。在等待期间新产生告警的状态为pending。 lables：自定义标签，允许用户指定要附加到告警上的一组附加标签 summary: 描述告警概要信息 annotations：用于指定一组附加信息，比如用于描述告警详细信息的文字等，annotations的内容在告警产生时会一同作为参数发送到Alertmanager 模块化，为了增加summary和annotation的可读性，prometheus可以通过$labels.变量可以访问当前告警实例中指定标签的值。$value则可以获取当前PromQL表达式计算的样本值。  # Alert for any instance that is unreachable for \u0026gt;5 minutes.\r- alert: InstanceDown\rexpr: up == 0\rfor: 5m\rlabels:\rseverity: page\rannotations:\rsummary: \u0026quot;Instance {{ $labels.instance }} down\u0026quot;\rdescription: \u0026quot;{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 5 minutes.\u0026quot;\r 通过promql alters可以查看当前报警，prometheus当满足告警规则之后，promethues会根据for字段定义的时间，从Pending状态变为Firing状态，这时候把报警信息传递给altermanager\nAlertManager alertmanager特性：   分组   分组机制可以将详细的告警信息合并成一个通知。在某些情况下，比如由于系统宕机导致大量的告警被同时触发，在这种情况下分组机制可以将这些被触发的告警合并为一个告警通知，避免一次性接受大量的告警通知，而无法对问题进行快速定位。\n例如，当集群中有数百个正在运行的服务实例，并且为每一个实例设置了告警规则。假如此时发生了网络故障，可能导致大量的服务实例无法连接到数据库，结果就会有数百个告警被发送到Alertmanager。\n而作为用户，可能只希望能够在一个通知中中就能查看哪些服务实例收到影响。这时可以按照服务所在集群或者告警名称对告警进行分组，而将这些告警内聚在一起成为一个通知。\n告警分组，告警时间，以及告警的接受方式可以通过Alertmanager的配置文件进行配置。\n  抑制   抑制是指当某一告警发出后，可以停止重复发送由此告警引发的其它告警的机制。\n例如，当集群不可访问时触发了一次告警，通过配置Alertmanager可以忽略与该集群有关的其它所有告警。这样可以避免接收到大量与实际问题无关的告警通知。\n抑制机制同样通过Alertmanager的配置文件进行设置。\n  静默   静默提供了一个简单的机制可以快速根据标签对告警进行静默处理。如果接收到的告警符合静默的配置，Alertmanager则不会发送告警通知。\n静默设置需要在Alertmanager的Werb页面上进行设置\nalertmanager配置说明： Alertmanager主要负责对Prometheus产生的告警进行统一处理，因此在Alertmanager配置中一般会包含以下几个主要部分：\n  全局配置（global）：用于定义一些全局的公共参数，如全局的SMTP配置，Slack配置等内容；\n  模板（templates）：用于定义告警通知时的模板，如HTML模板，邮件模板等；\n  告警路由（route）：根据标签匹配，确定当前告警应该如何处理；\n  接收人（receivers）：接收人是一个抽象的概念，它可以是一个邮箱也可以是微信，Slack或者Webhook等，接收人一般配合告警路由使用；\n  resolve_timeout: 当Alertmanager持续多长时间未接收到告警后标记告警状态为resolved（已解决）\n  抑制规则（inhibit_rules）：合理设置抑制规则可以减少垃圾告警的产生\nglobal:\r[ resolve_timeout: \u0026lt;duration\u0026gt; | default = 5m ]\r[ smtp_from: \u0026lt;tmpl_string\u0026gt; ] [ smtp_smarthost: \u0026lt;string\u0026gt; ] [ smtp_hello: \u0026lt;string\u0026gt; | default = \u0026quot;localhost\u0026quot; ]\r[ smtp_auth_username: \u0026lt;string\u0026gt; ]\r[ smtp_auth_password: \u0026lt;secret\u0026gt; ]\r[ smtp_auth_identity: \u0026lt;string\u0026gt; ]\r[ smtp_auth_secret: \u0026lt;secret\u0026gt; ]\r[ smtp_require_tls: \u0026lt;bool\u0026gt; | default = true ]\r[ slack_api_url: \u0026lt;secret\u0026gt; ]\r[ victorops_api_key: \u0026lt;secret\u0026gt; ]\r[ victorops_api_url: \u0026lt;string\u0026gt; | default = \u0026quot;https://alert.victorops.com/integrations/generic/20131114/alert/\u0026quot; ]\r[ pagerduty_url: \u0026lt;string\u0026gt; | default = \u0026quot;https://events.pagerduty.com/v2/enqueue\u0026quot; ]\r[ opsgenie_api_key: \u0026lt;secret\u0026gt; ]\r[ opsgenie_api_url: \u0026lt;string\u0026gt; | default = \u0026quot;https://api.opsgenie.com/\u0026quot; ]\r[ hipchat_api_url: \u0026lt;string\u0026gt; | default = \u0026quot;https://api.hipchat.com/\u0026quot; ]\r[ hipchat_auth_token: \u0026lt;secret\u0026gt; ]\r[ wechat_api_url: \u0026lt;string\u0026gt; | default = \u0026quot;https://qyapi.weixin.qq.com/cgi-bin/\u0026quot; ]\r[ wechat_api_secret: \u0026lt;secret\u0026gt; ]\r[ wechat_api_corp_id: \u0026lt;string\u0026gt; ]\r[ http_config: \u0026lt;http_config\u0026gt; ]\rtemplates:\r[ - \u0026lt;filepath\u0026gt; ... ]\rroute: \u0026lt;route\u0026gt;\rreceivers:\r- \u0026lt;receiver\u0026gt; ...\rinhibit_rules:\r[ - \u0026lt;inhibit_rule\u0026gt; ... ]\r 基于标签的告警处理路由: 基本配置如下：\nroute:\rgroup_by: ['alertname']\rreceiver: 'web.hook'\rreceivers:\r- name: 'web.hook'\rwebhook_configs:\r- url: 'http://127.0.0.1:5001/'\r [ receiver: \u0026lt;string\u0026gt; ]\r[ group_by: '[' \u0026lt;labelname\u0026gt;, ... ']' ]\r[ continue: \u0026lt;boolean\u0026gt; | default = false ]\rmatch:\r[ \u0026lt;labelname\u0026gt;: \u0026lt;labelvalue\u0026gt;, ... ]\rmatch_re:\r[ \u0026lt;labelname\u0026gt;: \u0026lt;regex\u0026gt;, ... ]\r[ group_wait: \u0026lt;duration\u0026gt; | default = 30s ]\r[ group_interval: \u0026lt;duration\u0026gt; | default = 5m ]\r[ repeat_interval: \u0026lt;duration\u0026gt; | default = 4h ]\rroutes:\r[ - \u0026lt;route\u0026gt; ... ]\r ","id":32,"section":"posts","summary":"[toc] promethues +altermanager 告警流程 如下图所示：通过prometheus定义的AlterRule，prometheus会周期性的对alterRules进行计算，当","tags":null,"title":"","uri":"https://zhangshunping.github.io/1/01/prometheus-rules/","year":"0001"},{"content":"[toc]\nThanos ","id":33,"section":"posts","summary":"[toc]\nThanos ","tags":null,"title":"","uri":"https://zhangshunping.github.io/1/01/prometheus-thanos/","year":"0001"},{"content":"vue指令 v-on 指令 : 为元素绑定事件 \u0026lt;div id=\u0026quot;app\u0026quot;\u0026gt;\r\u0026lt;input type=\u0026quot;button\u0026quot; value=\u0026quot;vue-on\u0026quot; @click=\u0026quot;doIt\u0026quot;\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;script\u0026gt;s\rnew Vue({\rel: '#app',\rmethods: {\rdoIt:function () {\ralert(\u0026quot;do it \u0026quot;)\r}\r}\r})\r\u0026lt;/script\u0026gt;\r   v-on 通过改变数据来实现dom\n\u0026lt;div id=\u0026quot;app\u0026quot;\u0026gt;\r\u0026lt;input type=\u0026quot;button\u0026quot; value=\u0026quot;ChangeMes\u0026quot; @click=\u0026quot;ChangeName\u0026quot;\u0026gt;\r\u0026lt;h1 v-text=\u0026quot;mes\u0026quot;\u0026gt;\u0026lt;/h1\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;script\u0026gt;\rnew Vue({\rel: '#app',\rdata: {\rmes: 'hello World!',\r},\rmethods:{\rChangeName:function () {\rthis.mes=\u0026quot;apple\u0026quot;\r}\r}\r})\r\u0026lt;/script\u0026gt;\r   本地应用   计数器\n\u0026lt;div id=\u0026quot;app\u0026quot; \u0026gt;\r\u0026lt;button @click=\u0026quot;sub\u0026quot;\u0026gt;-\u0026lt;/button\u0026gt;\r\u0026lt;span v-text=\u0026quot;mes\u0026quot;\u0026gt;\u0026lt;/span\u0026gt;\r\u0026lt;button @click=\u0026quot;add\u0026quot;\u0026gt;+\u0026lt;/button\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;script\u0026gt;\rnew Vue({\rel: '#app',\rdata: {\rmes: 1,\r},\rmethods:{\rsub:function () {\rif (this.mes\u0026gt;0){\rthis.mes-=1;\r}else{\ralert(this.mes+\u0026quot;已经是最小值0\u0026quot;)\r}\r},\radd:function () {\rif (this.mes\u0026lt;10){\rthis.mes+=1\r}else{\ralert(this.mes+\u0026quot;已经是最大值10\u0026quot;)\r}\r}\r}\r})\r\u0026lt;/script\u0026gt;\r   ","id":34,"section":"posts","summary":"vue指令 v-on 指令 : 为元素绑定事件 \u0026lt;div id=\u0026quot;app\u0026quot;\u0026gt; \u0026lt;input type=\u0026quot;button\u0026quot; value=\u0026quot;vue-on\u0026quot; @click=\u0026quot;doIt\u0026quot;\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;script\u0026gt;s new Vue({ el: '#app', methods: { doIt:function () { alert(\u0026quot;do it \u0026quot;) } } }) \u0026lt;/script\u0026gt; v-on 通过改变数据来实现dom \u0026lt;div id=\u0026quot;app\u0026quot;\u0026gt; \u0026lt;input type=\u0026quot;button\u0026quot; value=\u0026quot;ChangeMes\u0026quot; @click=\u0026quot;ChangeName\u0026quot;\u0026gt; \u0026lt;h1 v-text=\u0026quot;mes\u0026quot;\u0026gt;\u0026lt;/h1\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;script\u0026gt; new Vue({ el: '#app', data: { mes:","tags":null,"title":"","uri":"https://zhangshunping.github.io/1/01/vue%E4%B8%80/","year":"0001"}],"tags":[{"title":"awk","uri":"https://zhangshunping.github.io/tags/awk/"},{"title":"Gin","uri":"https://zhangshunping.github.io/tags/gin/"},{"title":"golang","uri":"https://zhangshunping.github.io/tags/golang/"},{"title":"golang-channel","uri":"https://zhangshunping.github.io/tags/golang-channel/"},{"title":"iptables","uri":"https://zhangshunping.github.io/tags/iptables/"},{"title":"kubernetes","uri":"https://zhangshunping.github.io/tags/kubernetes/"},{"title":"linux","uri":"https://zhangshunping.github.io/tags/linux/"},{"title":"operator","uri":"https://zhangshunping.github.io/tags/operator/"},{"title":"prometheus","uri":"https://zhangshunping.github.io/tags/prometheus/"},{"title":"Python","uri":"https://zhangshunping.github.io/tags/python/"},{"title":"rync","uri":"https://zhangshunping.github.io/tags/rync/"},{"title":"zabbix","uri":"https://zhangshunping.github.io/tags/zabbix/"},{"title":"监控","uri":"https://zhangshunping.github.io/tags/%E7%9B%91%E6%8E%A7/"}]}